{"cells":[{"cell_type":"markdown","source":"# Dask dataframes on HDFS\n\nTo use Dask dataframes in parallel across an HDFS cluster to read CSV data. We can coordinate these computations with [distributed](http://distributed.dask.org/en/latest/) and dask.dataframe.\n\nAs Spark, Dask can work in cluster mode. There is several ways to launch a cluster.","metadata":{"cell_id":"00000-527bc450-25b5-48e1-bf73-857c366f5001","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Local cluster","metadata":{"tags":[],"cell_id":"00001-8a5ef57b-e4fc-45af-aa30-8389de70bc3c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-a625f38b-4c20-409c-b827-196021dd4c47","output_cleared":false,"source_hash":"4e55e2c8","execution_start":1606918412381,"execution_millis":1048,"deepnote_cell_type":"code"},"source":"from dask.distributed import LocalCluster, Client\ncluster = LocalCluster()\ncluster","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 41037 instead\n  http_address[\"port\"], self.http_server.port\n","output_type":"stream"},{"data":{"text/plain":"VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6987a31124ae45e587bf1135a56b41b8"}},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","source":"## Remote clusters via SSH\n\nCode below can be used to launch a Dask SSH cluster on svmass2 server. \n\n```python\nfrom dask.distributed import SSHCluster\n\nsvpes = [f\"svpe{i:1d}\" for i in range(1,10)]\nprint(svpes)\ncluster = SSHCluster([\"localhost\"] + svpes)\ncluster\n```","metadata":{"tags":[],"cell_id":"00003-9246761f-912e-4a50-8f8e-e6f1bad38de0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Yarn cluster\n\nFollow these [instructions](https://yarn.dask.org/en/latest/environments.html) to create the environment file.\n\n```\nfrom dask_yarn import YarnCluster\nfrom dask.distributed import Client\n\n# Create a cluster where each worker has two cores and eight GiB of memory\ncluster = YarnCluster(environment='environment.tar.gz',\n                      worker_vcores=2,\n                      worker_memory=\"8GiB\")\n# Scale out to ten such workers\ncluster.scale(10)\n\n# Connect to the cluster\nclient = Client(cluster)\n```","metadata":{"tags":[],"cell_id":"00004-82aab717-71ff-46e7-92ff-2b5c44193ab6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## SLURM Cluster\n\nYou can use the dask module [dask_jobqueue](https://jobqueue.dask.org/en/latest/) \nto launch a Dask cluster with the job manager SLURM.\n\n```py\nfrom dask_jobqueue import SLURMCluster\n\ncluster = SLURMCluster(cores=16,\n             queue='test',\n             project='myproject',\n             memory=\"16GB\",\n             walltime=\"01:00:00\")\n```\n\nThe cluster generates a traditional job script and submits that an appropriate number of times to the job queue. You can see the job script that it will generate as follows:\n\n```py\nprint(cluster.job_script())\n```\n\n```bash\n#!/usr/bin/env bash\n\n#SBATCH -J dask-worker\n#SBATCH -p test\n#SBATCH -A myproject\n#SBATCH -n 1\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=15G\n#SBATCH -t 01:00:00\n\n/opt/tljh/user/envs/big-data/bin/python -m distributed.cli.dask_worker tcp://192.168.2.54:40623 --nthreads 4 --nprocs 4 --memory-limit 4.00GB --name name --nanny --death-timeout 60\n```\nUse the script above to submit your dask pipeline to the HPC server of your insttitution.","metadata":{"tags":[],"cell_id":"00001-c4c24411-83e5-48f3-9a5e-8bd403731783","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## New York City Taxi Cab Trip\n\nWe look at [the New York City Taxi Cab dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). This includes every ride made in the city of New York since 2009.\n\nOn [this website](http://chriswhong.github.io/nyctaxi/) you can see the data for one random NYC yellow taxi on a single day.\n\nOn [this post](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/), you can see an analysis of this dataset. Postgres and R scripts are available on [GitHub](https://github.com/toddwschneider/nyc-taxi-data). There is also a dashboard available [here](https://toddwschneider.com/dashboards/nyc-taxi-ridehailing-uber-lyft-data/) that updates monthly with the latest taxi, Uber, and Lyft aggregate stats.\n\n","metadata":{"cell_id":"00005-2b67d07a-67bd-4c56-9869-96681e67b32e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00011-580f4583-2389-4de5-926a-3e4190b8585b","deepnote_cell_type":"code"},"source":"```python\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n\n`nyc2014` is a dask.dataframe objects which present a subset of the Pandas API to the user, but farm out all of the work to the many Pandas dataframes they control across the network.\n\nnyc2014 = dd.read_csv('/opt/datasets/nyc-data/2014/yellow*.csv',\nparse_dates=['pickup_datetime', 'dropoff_datetime'],\nskipinitialspace=True)\nnyc2014 = c.persist(nyc2014)\nprogress(nyc2014)\n```","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercises \n\n- Display head of the dataframe\n- Display number of rows of this dataframe.\n- Compute the total number of passengers.\n- Count occurrences in the payment_type column both for the full dataset, and filtered by zero tip (tip_amount == 0).\n- Create a new column, tip_fraction\n- Plot the average of the new column tip_fraction grouped by day of week.\n- Plot the average of the new column tip_fraction grouped by hour of day.\n\n[Dask dataframe documentation](http://docs.dask.org/en/latest/dataframe.html)","metadata":{"tags":[],"cell_id":"00013-61cbcc92-4356-437b-adac-ea5609c5df9d","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"big-data","language":"python","name":"big-data"},"deepnote_notebook_id":"379fcb20-a410-4e6b-9597-3e0eecd8481f","deepnote_execution_queue":[]}}