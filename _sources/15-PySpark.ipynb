{"cells":[{"cell_type":"markdown","source":"# PySpark\n\n![Logo](images/apache_spark_logo.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00000-a579b3bd-6eb6-4445-8bb9-82fb1dc1e072"}},{"cell_type":"markdown","source":"- [Apache Spark](https://spark.apache.org) was first released in 2014. \n- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n- Spark is written in [Scala](https://www.scala-lang.org).\n- All images come from [Databricks](https://databricks.com/product/getting-started-guide).","metadata":{"deepnote_cell_type":"markdown","cell_id":"00001-bf31e048-ace9-4993-8da2-5f1af4015c57"}},{"cell_type":"markdown","source":"- Apache Spark is a fast and general-purpose cluster computing system. \n- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n- Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n- It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and Spark Streaming.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00002-bb8dce6d-d8e9-486b-818d-f6dcc250b446"}},{"cell_type":"markdown","source":"## Resilient distributed datasets\n\n- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n- RDDs behave a bit like Python collections (e.g. lists).\n- When working with Apache Spark we iteratively apply functions to every item of these collections in parallel to produce *new* RDDs.\n- The data is distributed across nodes in a cluster of computers.\n- Functions implemented in Spark can work in parallel across elements of the collection.\n- The  Spark framework allocates data and processing to different nodes, without any intervention from the programmer.\n- RDDs automatically rebuilt on machine failure.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00003-d67dd6f1-d704-4caa-81cf-5a9071b62f17"}},{"cell_type":"markdown","source":"## Lifecycle of a Spark Program\n\n1. Create some input RDDs from external data or parallelize a collection in your driver program.\n2. Lazily transform them to define new RDDs using transformations like `filter()` or `map()`\n3. Ask Spark to cache() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00004-befe75de-d238-405e-81e7-38cedbf082ce"}},{"cell_type":"markdown","source":"## Operations on Distributed Data\n\n- Two types of operations: **transformations** and **actions**\n- Transformations are *lazy* (not computed immediately) \n- Transformations are executed when an action is run","metadata":{"deepnote_cell_type":"markdown","cell_id":"00005-7b21b99e-f856-4c8e-9be8-3c9e9f4e9630"}},{"cell_type":"markdown","source":"## [Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n\n```\nmap() flatMap()\nfilter() \nmapPartitions() mapPartitionsWithIndex() \nsample()\nunion() intersection() distinct()\ngroupBy() groupByKey()\nreduceBy() reduceByKey()\nsortBy() sortByKey()\njoin()\ncogroup()\ncartesian()\npipe()\ncoalesce()\nrepartition()\npartitionBy()\n...\n```","metadata":{"deepnote_cell_type":"markdown","cell_id":"00006-d114d429-bb93-4c93-affc-216589f7db5e"}},{"cell_type":"markdown","source":"## [Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n\n```\nreduce()\ncollect()\ncount()\nfirst()\ntake()\ntakeSample()\nsaveToCassandra()\ntakeOrdered()\nsaveAsTextFile()\nsaveAsSequenceFile()\nsaveAsObjectFile()\ncountByKey()\nforeach()\n```","metadata":{"deepnote_cell_type":"markdown","cell_id":"00007-c19d8245-7c9f-407b-8518-0cdbb557be79"}},{"cell_type":"markdown","source":"## Python API\n\nPySpark uses Py4J that enables Python programs to dynamically access Java objects.\n\n![PySpark Internals](images/YlI8AqEl.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00008-385816d2-d5dd-4556-a0ec-2bbc0d0f54dc"}},{"cell_type":"markdown","source":"## The `SparkContext` class\n\n- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.SparkContext` context.\n\n- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n\n- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n    - normally we would create an RDD from a large file or an HBase table.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00009-f66ff6fd-edde-4c4c-bfa1-ba0d1d96ba97"}},{"cell_type":"markdown","source":"## First example\n\nPySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter.\n\nWe have a spark context sc to use with a tiny local spark cluster with 4 nodes (will work just fine on a multicore machine).","metadata":{"deepnote_cell_type":"markdown","cell_id":"00010-a1e30157-0efd-490d-8b63-459dfb96da8a"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00012-8698152c-385e-4d22-ae56-408429b08600","output_cleared":false,"source_hash":"a22d9657","execution_millis":4,"execution_start":1606208122779},"source":"import os, sys\nsys.executable","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'/opt/venv/bin/python'"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00013-7e179656-f878-4ef7-bfc5-4683fd92851d","output_cleared":false,"source_hash":"a04577ae","execution_millis":1,"execution_start":1606208260172},"source":"os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.0.1-bin-hadoop2.7\"\nos.environ[\"PYSPARK_PYTHON\"] = sys.executable","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00014-f661aeba-0a4e-489c-93b7-034ee40d1ef4","output_cleared":false,"source_hash":"73659aa7","execution_millis":7637,"execution_start":1606208269332},"source":"import pyspark\n\nsc = pyspark.SparkContext(master=\"local[*]\", appName=\"FirstExample\")\nsc.setLogLevel(\"ERROR\")","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00015-bf748379-3532-453e-8058-f4fc6531a830","output_cleared":false,"source_hash":"3831e90d","execution_millis":5,"execution_start":1606208288902},"source":"print(sc) # it is like a Pool Processor executor","execution_count":7,"outputs":[{"name":"stdout","text":"<SparkContext master=local[*] appName=FirstExample>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create your first RDD","metadata":{"deepnote_cell_type":"markdown","cell_id":"00016-192e38d9-530b-4b44-82f2-0e98ab5d8379"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00017-6079893b-b1de-4a98-91ee-cbc10b34cb56","output_cleared":false,"source_hash":"27883792","execution_millis":549,"execution_start":1606208314201},"source":"data = list(range(8))\nrdd = sc.parallelize(data) # create collection\nrdd","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exercise\n\nCreate a file `sample.txt`with lorem package. Read and load it into a RDD with the `textFile` spark function.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00018-3d4cb001-a63e-4efa-a375-656a819b62f7"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00019-2d925cb0-f03e-475c-8f9f-59d9930824b3","output_cleared":false,"source_hash":"6dc5b4c8","execution_millis":104,"execution_start":1606208752430},"source":"from faker import Faker\nfake = Faker()\nFaker.seed(0)\n\nwith open(\"sample.txt\",\"w\") as f:\n    f.write(fake.text(max_nb_chars=1000))\n    \nrdd = sc.textFile(\"sample.txt\")","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Collect\n\nAction / To Driver: Return all items in the RDD to the driver in a single list\n\n![](images/DUO6ygB.png)\n\nSource: https://i.imgur.com/DUO6ygB.png\n\n### Exercise \n\nCollect the text you read before from the `sample.txt`file.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00020-f8161a29-5442-40bc-b3ad-314ac079b925"}},{"cell_type":"markdown","source":"### Map\n\nTransformation / Narrow: Return a new RDD by applying a function to each element of this RDD\n\n![](images/PxNJf0U.png)\n\nSource: http://i.imgur.com/PxNJf0U.png","metadata":{"deepnote_cell_type":"markdown","cell_id":"00021-88914dd9-d7ed-435c-988d-b333620a63f9"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00022-e19738f1-5568-4e01-a918-31f95720f79a","output_cleared":false,"source_hash":"8210a3cb","execution_start":1605183503558,"execution_millis":2412},"source":"rdd = sc.parallelize(list(range(8)))\nrdd.map(lambda x: x ** 2).collect() # Square each element","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"[0, 1, 4, 9, 16, 25, 36, 49]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exercise\n\nReplace the lambda function by a function that contains a pause (sleep(1)) and check if the `map` operation is parallelized.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00023-096cde31-3814-4827-a025-e528f1021b55"}},{"cell_type":"markdown","source":"### Filter\n\nTransformation / Narrow: Return a new RDD containing only the elements that satisfy a predicate\n\n![](images/GFyji4U.png)\nSource: http://i.imgur.com/GFyji4U.png","metadata":{"deepnote_cell_type":"markdown","cell_id":"00024-0a7d639a-570b-49d3-a71a-7330ec203bc8"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00025-9893a281-e7fd-49e4-880e-c18c223437e2"},"source":"# Select only the even elements\nrdd.filter(lambda x: x % 2 == 0).collect()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FlatMap\n\nTransformation / Narrow: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n\n![](images/TsSUex8.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00026-c2044f27-66c2-49e4-a2c3-1551769fc2fe"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00027-eab04ee2-2189-474e-b0ee-7cdf04d51334","output_cleared":false,"source_hash":"ed69af29","execution_start":1606209096798,"execution_millis":1423},"source":"rdd = sc.parallelize([1,2,3])\nrdd.flatMap(lambda x: (x, x*100, 42)).collect()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[1, 100, 42, 2, 200, 42, 3, 300, 42]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exercise\n\nUse FlatMap to clean the text from `sample.txt`file. Lower, remove dots and split into words.\n\n### GroupBy\n\nTransformation / Wide: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.\n\n![](images/gdj0Ey8.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00028-d3078029-dd5d-4852-a84f-4fb134597c5d"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00029-93936a32-6fc4-4c60-b38d-7935aa2df61f"},"source":"rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\nrdd = rdd.groupBy(lambda w: w[0])\n[(k, list(v)) for (k, v) in rdd.collect()]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GroupByKey\n\nTransformation / Wide: Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.\n\n![](images/TlWRGr2.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00030-dd9efd67-bc72-4eb5-9e07-0a94bd233e13"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00031-bc1d6a4c-8103-42a9-a39a-ee8d80680447"},"source":"rdd = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\nrdd = rdd.groupByKey()\n[(j[0], list(j[1])) for j in rdd.collect()]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join\n\nTransformation / Wide: Return a new RDD containing all pairs of elements having the same key in the original RDDs\n\n![](images/YXL42Nl.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00032-6cff9fda-3f13-4b2b-9b2e-580ad83ecf5f"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00033-10e3b4f2-3f6f-4a99-adac-d8bc8c92c2e9"},"source":"x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\ny = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\nx.join(y).collect()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distinct\n\nTransformation / Wide: Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n\n![](images/Vqgy2a4.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00034-7746caa4-cfd5-4206-b12d-f0fe01d65803"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00035-fd077e6b-1166-4fca-b5e8-c9a310dc59e7"},"source":"rdd = sc.parallelize([1,2,3,3,4])\nrdd.distinct().collect()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KeyBy\n\nTransformation / Narrow: Create a Pair RDD, forming one pair for each item in the original RDD. The pairâ€™s key is calculated from the value via a user-supplied function.\n\n![](images/nqYhDW5.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00036-fdbf2aa3-158f-44a2-82a3-fab846eb5ab7"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00037-e13e8ce6-ad59-4cb9-a672-336afba96873"},"source":"rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\nrdd.keyBy(lambda w: w[0]).collect()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Actions\n\n### Map-Reduce operation \n\nAction / To Driver: Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and return a result to the driver\n\n![](images/R72uzwX.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00038-ee1a58a6-954d-4994-a48a-d5a98ac09051"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00039-cc090acd-104b-4683-844f-153af8f2956d"},"source":"from operator import add\nrdd = sc.parallelize(list(range(8)))\nrdd.map(lambda x: x ** 2).reduce(add) # reduce is an action!","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Max, Min, Sum, Mean, Variance, Stdev\n\nAction / To Driver: Compute the respective function (maximum value, minimum value, sum, mean, variance, or standard deviation) from a numeric RDD\n\n![](images/HUCtib1.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00040-a9e8e4e8-143f-403e-b11c-67b97832198b"}},{"cell_type":"markdown","source":"### CountByKey\n\nAction / To Driver: Return a map of keys and counts of their occurrences in the RDD\n\n![](images/jvQTGv6.png)","metadata":{"deepnote_cell_type":"markdown","cell_id":"00041-e22cf358-c375-48c8-ba0b-5489d848df67"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00042-613102f2-fea1-436a-a695-8169201af599"},"source":"rdd = sc.parallelize([('J', 'James'), ('F','Fred'), \n                    ('A','Anna'), ('J','John')])\n\nrdd.countByKey()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00043-a8624fda-21b4-4d84-b41a-0b2f94951a4a"},"source":"# Stop the local spark cluster\nsc.stop()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 10.1 Word-count in Apache Spark\n\n- Write the sample text file","metadata":{"deepnote_cell_type":"markdown","cell_id":"00044-36e25a83-4df8-4b26-b143-ac3cf849110a"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00045-5b333640-61a1-44e1-b743-aa57a5042686"},"source":"from lorem import text\nwith open('sample.txt','w') as f:\n    f.write(text())","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Create the rdd with `SparkContext.textFile method`\n- lower, remove dots and split using `rdd.flatMap`\n- use `rdd.map` to create the list of key/value pair (word, 1)\n- `rdd.reduceByKey` to get all occurences\n- `rdd.takeOrdered`to get sorted frequencies of words\n\nAll documentation is available [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext) for textFile and [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.RDD) for RDD. \n\nFor a global overview see the Transformations section of the [programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n\n\n## SparkSession\n\nSince SPARK 2.0.0,  SparkSession provides a single point \nof entry to interact with Spark functionality and\nallows programming Spark with DataFrame and Dataset APIs. \n\n###  $\\pi$ computation example\n\n- We can estimate an approximate value for $\\pi$ using the following Monte-Carlo method:\n\n1.    Inscribe a circle in a square\n2.    Randomly generate points in the square\n3.    Determine the number of points in the square that are also in the circle\n4.    Let $r$ be the number of points in the circle divided by the number of points in the square, then $\\pi \\approx 4 r$.\n    \n- Note that the more points generated, the better the approximation\n\nSee [this tutorial](https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI).\n\n\n### Exercise 9.2\n\nUsing the same method than the PI computation example, compute the integral\n$$\nI = \\int_0^1 \\exp(-x^2) dx\n$$\nYou can check your result with numpy","metadata":{"deepnote_cell_type":"markdown","cell_id":"00046-4fe65a61-99c3-41b6-b52c-4995e5992182"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00047-4e2ae5f8-be58-401c-8691-41d0eb0e7932"},"source":"# numpy evaluates solution using numeric computation. \n# It uses discrete values of the function\nimport numpy as np\nx = np.linspace(0,1,1000)\nnp.trapz(np.exp(-x*x),x)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"numpy and scipy evaluates solution using numeric computation. It uses discrete values of the function","metadata":{"deepnote_cell_type":"markdown","cell_id":"00048-8ed1b85e-33b8-477f-81dd-a97921ced018"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00049-f4fbd4ea-4a0c-41fb-81cd-e319fee06fd5"},"source":"import numpy as np\nfrom scipy.integrate import quad\nquad(lambda x: np.exp(-x*x), 0, 1)\n# note: the solution returned is complex ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation between daily stock\n\n- Data preparation","metadata":{"deepnote_cell_type":"markdown","cell_id":"00050-43ed89d4-eb36-4fc8-99fe-640ba0d5f4b7"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00051-c66a61e1-30d9-4d8f-8d8c-06e39894d019"},"source":"import os  # library to get directory and file paths\nimport tarfile # this module makes possible to read and write tar archives\n\ndef extract_data(name, where):\n    datadir = os.path.join(where,name)\n    if not os.path.exists(datadir):\n       print(\"Extracting data...\")\n       tar_path = os.path.join(where, name+'.tgz')\n       with tarfile.open(tar_path, mode='r:gz') as data:\n          data.extractall(where)\n            \nextract_data('daily-stock','data') # this function call will extract json files","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00052-5dba8c02-1524-4ef4-b5f5-85a65e1434c5"},"source":"import json\nimport pandas as pd\nimport os, glob\n\nhere = os.getcwd()\ndatadir = os.path.join(here,'data','daily-stock')\nfilenames = sorted(glob.glob(os.path.join(datadir, '*.json')))\nfilenames","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00053-6816ac9f-f87e-4f04-956c-105517c73273"},"source":"%rm data/daily-stock/*.h5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00054-c7da4efd-ed13-469f-a26b-2f6e285603f7"},"source":"from glob import glob\nimport os, json\nimport pandas as pd\n\nfor fn in filenames:\n    with open(fn) as f:\n        data = [json.loads(line) for line in f]\n        \n    df = pd.DataFrame(data)\n    \n    out_filename = fn[:-5] + '.h5'\n    df.to_hdf(out_filename, '/data')\n    print(\"Finished : %s\" % out_filename.split(os.path.sep)[-1])\n\nfilenames = sorted(glob(os.path.join('data', 'daily-stock', '*.h5')))  # data/json/*.json","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequential code","metadata":{"deepnote_cell_type":"markdown","cell_id":"00055-ee42b11a-a425-4012-a804-10338e33e432"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00056-114f9f2a-37a4-4414-a5dd-c2a6bf1f3835"},"source":"filenames","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00057-39e60f20-3194-4f82-8230-9dbef224afa8"},"source":"with pd.HDFStore('data/daily-stock/aet.h5') as hdf:\n    # This prints a list of all group names:\n    print(hdf.keys())","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00058-957e72f9-0bea-498d-93e2-685cea013bd5"},"source":"df_test = pd.read_hdf('data/daily-stock/aet.h5')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00059-ae84d087-9d8b-4288-bc0d-aad10643fb19"},"source":"%%time\n\nseries = []\nfor fn in filenames:   # Simple map over filenames\n    series.append(pd.read_hdf(fn)[\"close\"])\n\nresults = []\n\nfor a in series:    # Doubly nested loop over the same collection\n    for b in series:  \n        if not (a == b).all():     # Filter out comparisons of the same series \n            results.append(a.corr(b))  # Apply function\n\nresult = max(results)\nresult","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercise 9.3\n\nParallelize the code above with Apache Spark.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00060-32f98a90-1082-4333-bcd8-e6d80f37fea2"}},{"cell_type":"markdown","source":"- Change the filenames because of the Hadoop environment.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00061-029b8755-d3bc-4074-aa42-f6307b99fe82"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00062-06454d0f-52e6-4f2e-abd5-be1ece9f3cbb"},"source":"import os, glob\n\nhere = os.getcwd()\nfilenames = sorted(glob.glob(os.path.join(here,'data', 'daily-stock', '*.h5')))\nfilenames","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If it is not started don't forget the PySpark context","metadata":{"deepnote_cell_type":"markdown","cell_id":"00063-47d826ff-dcd4-433d-acfa-19d8c3866425"}},{"cell_type":"markdown","source":"Computation time is slower because there is a lot of setup, workers creation, there is a lot of communications the correlation function is too small\n\n### Exercise 9.4 Fasta file example\n\nUse a RDD to calculate the GC content of fasta file nucleotide-sample.txt:\n\n$$\\frac{G+C}{A+T+G+C}\\times100 \\% $$\n\nCreate a rdd from fasta file genome.txt in data directory and count 'G' and 'C' then divide by the total number of bases.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00064-03268a38-d5fe-43a3-b5eb-d6e3b081b199"}},{"cell_type":"markdown","source":"### Another example\n\nCompute the most frequent sequence with 5 bases.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00065-d670cabd-57f3-478f-8c7c-d5f1f9bbaf0a"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"jupytext":{"formats":"ipynb,md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":"0.9","jupytext_version":"1.5.2"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"source_map":[13,19,26,33,45,54,62,85,105,113,124,134,143,148,153,160,162,166,170,176,183,197,207,210,216,225,228,236,239,251,255,263,267,275,279,287,290,298,301,311,315,323,331,338,341,347,351,392,398,402,407,413,428,439,443,459,463,467,473,477,493,499,503,509,513,525],"deepnote_notebook_id":"53ba4a86-7d9e-483d-acb5-582d7555be2b","deepnote_execution_queue":[]}}