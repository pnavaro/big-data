{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a579b3bd-6eb6-4445-8bb9-82fb1dc1e072",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# PySpark\n",
    "\n",
    "![Logo](images/apache_spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-bf31e048-ace9-4993-8da2-5f1af4015c57",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- [Apache Spark](https://spark.apache.org) was first released in 2014. \n",
    "- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n",
    "- Spark is written in [Scala](https://www.scala-lang.org).\n",
    "- All images come from [Databricks](https://databricks.com/product/getting-started-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-bb8dce6d-d8e9-486b-818d-f6dcc250b446",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- Apache Spark is a fast and general-purpose cluster computing system. \n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "- Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "- It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-d67dd6f1-d704-4caa-81cf-5a9071b62f17",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Resilient distributed datasets\n",
    "\n",
    "- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n",
    "- RDDs behave a bit like Python collections (e.g. lists).\n",
    "- When working with Apache Spark we iteratively apply functions to every item of these collections in parallel to produce *new* RDDs.\n",
    "- The data is distributed across nodes in a cluster of computers.\n",
    "- Functions implemented in Spark can work in parallel across elements of the collection.\n",
    "- The  Spark framework allocates data and processing to different nodes, without any intervention from the programmer.\n",
    "- RDDs automatically rebuilt on machine failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-befe75de-d238-405e-81e7-38cedbf082ce",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Lifecycle of a Spark Program\n",
    "\n",
    "1. Create some input RDDs from external data or parallelize a collection in your driver program.\n",
    "2. Lazily transform them to define new RDDs using transformations like `filter()` or `map()`\n",
    "3. Ask Spark to cache() any intermediate RDDs that will need to be reused.\n",
    "4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-7b21b99e-f856-4c8e-9be8-3c9e9f4e9630",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Operations on Distributed Data\n",
    "\n",
    "- Two types of operations: **transformations** and **actions**\n",
    "- Transformations are *lazy* (not computed immediately) \n",
    "- Transformations are executed when an action is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-d114d429-bb93-4c93-affc-216589f7db5e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## [Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n",
    "\n",
    "```\n",
    "map() flatMap()\n",
    "filter() \n",
    "mapPartitions() mapPartitionsWithIndex() \n",
    "sample()\n",
    "union() intersection() distinct()\n",
    "groupBy() groupByKey()\n",
    "reduceBy() reduceByKey()\n",
    "sortBy() sortByKey()\n",
    "join()\n",
    "cogroup()\n",
    "cartesian()\n",
    "pipe()\n",
    "coalesce()\n",
    "repartition()\n",
    "partitionBy()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-c19d8245-7c9f-407b-8518-0cdbb557be79",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## [Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "\n",
    "```\n",
    "reduce()\n",
    "collect()\n",
    "count()\n",
    "first()\n",
    "take()\n",
    "takeSample()\n",
    "saveToCassandra()\n",
    "takeOrdered()\n",
    "saveAsTextFile()\n",
    "saveAsSequenceFile()\n",
    "saveAsObjectFile()\n",
    "countByKey()\n",
    "foreach()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-385816d2-d5dd-4556-a0ec-2bbc0d0f54dc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Python API\n",
    "\n",
    "PySpark uses Py4J that enables Python programs to dynamically access Java objects.\n",
    "\n",
    "![PySpark Internals](images/YlI8AqEl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-f66ff6fd-edde-4c4c-bfa1-ba0d1d96ba97",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## The `SparkContext` class\n",
    "\n",
    "- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.SparkContext` context.\n",
    "\n",
    "- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n",
    "\n",
    "- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n",
    "    - normally we would create an RDD from a large file or an HBase table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-a1e30157-0efd-490d-8b63-459dfb96da8a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## First example\n",
    "\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter.\n",
    "\n",
    "We have a spark context sc to use with a tiny local spark cluster with 4 nodes (will work just fine on a multicore machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00012-8698152c-385e-4d22-ae56-408429b08600",
    "deepnote_cell_type": "code",
    "execution_millis": 4,
    "execution_start": 1606208122779,
    "output_cleared": false,
    "source_hash": "a22d9657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/big-data/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00013-7e179656-f878-4ef7-bfc5-4683fd92851d",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1606208260172,
    "output_cleared": false,
    "source_hash": "a04577ae"
   },
   "outputs": [],
   "source": [
    "#os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00014-f661aeba-0a4e-489c-93b7-034ee40d1ef4",
    "deepnote_cell_type": "code",
    "execution_millis": 7637,
    "execution_start": 1606208269332,
    "output_cleared": false,
    "source_hash": "73659aa7"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(master=\"local[*]\", appName=\"FirstExample\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00015-bf748379-3532-453e-8058-f4fc6531a830",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1606208288902,
    "output_cleared": false,
    "source_hash": "3831e90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=FirstExample>\n"
     ]
    }
   ],
   "source": [
    "print(sc) # it is like a Pool Processor executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-192e38d9-530b-4b44-82f2-0e98ab5d8379",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Create your first RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00017-6079893b-b1de-4a98-91ee-cbc10b34cb56",
    "deepnote_cell_type": "code",
    "execution_millis": 549,
    "execution_start": 1606208314201,
    "output_cleared": false,
    "source_hash": "27883792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(range(8))\n",
    "rdd = sc.parallelize(data) # create collection\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-3d4cb001-a63e-4efa-a375-656a819b62f7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Create a file `sample.txt`with lorem package. Read and load it into a RDD with the `textFile` spark function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00019-2d925cb0-f03e-475c-8f9f-59d9930824b3",
    "deepnote_cell_type": "code",
    "execution_millis": 104,
    "execution_start": 1606208752430,
    "output_cleared": false,
    "source_hash": "6dc5b4c8"
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "Faker.seed(0)\n",
    "\n",
    "with open(\"sample.txt\",\"w\") as f:\n",
    "    f.write(fake.text(max_nb_chars=1000))\n",
    "    \n",
    "rdd = sc.textFile(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-f8161a29-5442-40bc-b3ad-314ac079b925",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Collect\n",
    "\n",
    "Action / To Driver: Return all items in the RDD to the driver in a single list\n",
    "\n",
    "![](images/DUO6ygB.png)\n",
    "\n",
    "Source: https://i.imgur.com/DUO6ygB.png\n",
    "\n",
    "### Exercise \n",
    "\n",
    "Collect the text you read before from the `sample.txt`file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-88914dd9-d7ed-435c-988d-b333620a63f9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Map\n",
    "\n",
    "Transformation / Narrow: Return a new RDD by applying a function to each element of this RDD\n",
    "\n",
    "![](images/PxNJf0U.png)\n",
    "\n",
    "Source: http://i.imgur.com/PxNJf0U.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00022-e19738f1-5568-4e01-a918-31f95720f79a",
    "deepnote_cell_type": "code",
    "execution_millis": 2412,
    "execution_start": 1605183503558,
    "output_cleared": false,
    "source_hash": "8210a3cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(range(8)))\n",
    "rdd.map(lambda x: x ** 2).collect() # Square each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-096cde31-3814-4827-a025-e528f1021b55",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Replace the lambda function by a function that contains a pause (sleep(1)) and check if the `map` operation is parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-0a7d639a-570b-49d3-a71a-7330ec203bc8",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Filter\n",
    "\n",
    "Transformation / Narrow: Return a new RDD containing only the elements that satisfy a predicate\n",
    "\n",
    "![](images/GFyji4U.png)\n",
    "Source: http://i.imgur.com/GFyji4U.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00025-9893a281-e7fd-49e4-880e-c18c223437e2",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-c2044f27-66c2-49e4-a2c3-1551769fc2fe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### FlatMap\n",
    "\n",
    "Transformation / Narrow: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n",
    "\n",
    "![](images/TsSUex8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00027-eab04ee2-2189-474e-b0ee-7cdf04d51334",
    "deepnote_cell_type": "code",
    "execution_millis": 1423,
    "execution_start": 1606209096798,
    "output_cleared": false,
    "source_hash": "ed69af29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100, 42, 2, 200, 42, 3, 300, 42]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.flatMap(lambda x: (x, x*100, 42)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-d3078029-dd5d-4852-a84f-4fb134597c5d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Use FlatMap to clean the text from `sample.txt`file. Lower, remove dots and split into words.\n",
    "\n",
    "### GroupBy\n",
    "\n",
    "Transformation / Wide: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.\n",
    "\n",
    "![](images/gdj0Ey8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00029-93936a32-6fc4-4c60-b38d-7935aa2df61f",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('J', ['John', 'James']), ('F', ['Fred']), ('A', ['Anna'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "rdd = rdd.groupBy(lambda w: w[0])\n",
    "[(k, list(v)) for (k, v) in rdd.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-dd9efd67-bc72-4eb5-9e07-0a94bd233e13",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### GroupByKey\n",
    "\n",
    "Transformation / Wide: Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.\n",
    "\n",
    "![](images/TlWRGr2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00031-bc1d6a4c-8103-42a9-a39a-ee8d80680447",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [5, 4]), ('A', [3, 2, 1])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "rdd = rdd.groupByKey()\n",
    "[(j[0], list(j[1])) for j in rdd.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-6cff9fda-3f13-4b2b-9b2e-580ad83ecf5f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Join\n",
    "\n",
    "Transformation / Wide: Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
    "\n",
    "![](images/YXL42Nl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00033-10e3b4f2-3f6f-4a99-adac-d8bc8c92c2e9",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3)), ('a', (1, 4)), ('b', (2, 5))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00034-7746caa4-cfd5-4206-b12d-f0fe01d65803",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Distinct\n",
    "\n",
    "Transformation / Wide: Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n",
    "\n",
    "![](images/Vqgy2a4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00035-fd077e6b-1166-4fca-b5e8-c9a310dc59e7",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,3,4])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-fdbf2aa3-158f-44a2-82a3-fab846eb5ab7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### KeyBy\n",
    "\n",
    "Transformation / Narrow: Create a Pair RDD, forming one pair for each item in the original RDD. The pair’s key is calculated from the value via a user-supplied function.\n",
    "\n",
    "![](images/nqYhDW5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00037-e13e8ce6-ad59-4cb9-a672-336afba96873",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('J', 'John'), ('F', 'Fred'), ('A', 'Anna'), ('J', 'James')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "rdd.keyBy(lambda w: w[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-ee1a58a6-954d-4994-a48a-d5a98ac09051",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Actions\n",
    "\n",
    "### Map-Reduce operation \n",
    "\n",
    "Action / To Driver: Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and return a result to the driver\n",
    "\n",
    "![](images/R72uzwX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00039-cc090acd-104b-4683-844f-153af8f2956d",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize(list(range(8)))\n",
    "rdd.map(lambda x: x ** 2).reduce(add) # reduce is an action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-a9e8e4e8-143f-403e-b11c-67b97832198b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Max, Min, Sum, Mean, Variance, Stdev\n",
    "\n",
    "Action / To Driver: Compute the respective function (maximum value, minimum value, sum, mean, variance, or standard deviation) from a numeric RDD\n",
    "\n",
    "![](images/HUCtib1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-e22cf358-c375-48c8-ba0b-5489d848df67",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### CountByKey\n",
    "\n",
    "Action / To Driver: Return a map of keys and counts of their occurrences in the RDD\n",
    "\n",
    "![](images/jvQTGv6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00042-613102f2-fea1-436a-a695-8169201af599",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'J': 2, 'F': 1, 'A': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('J', 'James'), ('F','Fred'), \n",
    "                    ('A','Anna'), ('J','John')])\n",
    "\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "00043-a8624fda-21b4-4d84-b41a-0b2f94951a4a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Stop the local spark cluster\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00044-36e25a83-4df8-4b26-b143-ac3cf849110a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercise 10.1 Word-count in Apache Spark\n",
    "\n",
    "- Write the sample text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "00045-5b333640-61a1-44e1-b743-aa57a5042686",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "with open('sample.txt','w') as f:\n",
    "    f.write(text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-4fe65a61-99c3-41b6-b52c-4995e5992182",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- Create the rdd with `SparkContext.textFile method`\n",
    "- lower, remove dots and split using `rdd.flatMap`\n",
    "- use `rdd.map` to create the list of key/value pair (word, 1)\n",
    "- `rdd.reduceByKey` to get all occurences\n",
    "- `rdd.takeOrdered`to get sorted frequencies of words\n",
    "\n",
    "All documentation is available [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext) for textFile and [here](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=textfile#pyspark.RDD) for RDD. \n",
    "\n",
    "For a global overview see the Transformations section of the [programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "Since SPARK 2.0.0,  SparkSession provides a single point \n",
    "of entry to interact with Spark functionality and\n",
    "allows programming Spark with DataFrame and Dataset APIs. \n",
    "\n",
    "###  $\\pi$ computation example\n",
    "\n",
    "- We can estimate an approximate value for $\\pi$ using the following Monte-Carlo method:\n",
    "\n",
    "1.    Inscribe a circle in a square\n",
    "2.    Randomly generate points in the square\n",
    "3.    Determine the number of points in the square that are also in the circle\n",
    "4.    Let $r$ be the number of points in the circle divided by the number of points in the square, then $\\pi \\approx 4 r$.\n",
    "    \n",
    "- Note that the more points generated, the better the approximation\n",
    "\n",
    "See [this tutorial](https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI).\n",
    "\n",
    "\n",
    "### Exercise 9.2\n",
    "\n",
    "Using the same method than the PI computation example, compute the integral\n",
    "$$\n",
    "I = \\int_0^1 \\exp(-x^2) dx\n",
    "$$\n",
    "You can check your result with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "00047-4e2ae5f8-be58-401c-8691-41d0eb0e7932",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7468240713763741"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy evaluates solution using numeric computation. \n",
    "# It uses discrete values of the function\n",
    "import numpy as np\n",
    "x = np.linspace(0,1,1000)\n",
    "np.trapz(np.exp(-x*x),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00048-8ed1b85e-33b8-477f-81dd-a97921ced018",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "numpy and scipy evaluates solution using numeric computation. It uses discrete values of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "00049-f4fbd4ea-4a0c-41fb-81cd-e319fee06fd5",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7468241328124271, 8.291413475940725e-15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "quad(lambda x: np.exp(-x*x), 0, 1)\n",
    "# note: the solution returned is complex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00050-43ed89d4-eb36-4fc8-99fe-640ba0d5f4b7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Correlation between daily stock\n",
    "\n",
    "- Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "00051-c66a61e1-30d9-4d8f-8d8c-06e39894d019",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data...\n"
     ]
    }
   ],
   "source": [
    "import os  # library to get directory and file paths\n",
    "import tarfile # this module makes possible to read and write tar archives\n",
    "\n",
    "def extract_data(name, where):\n",
    "    datadir = os.path.join(where,name)\n",
    "    if not os.path.exists(datadir):\n",
    "       print(\"Extracting data...\")\n",
    "       tar_path = os.path.join(where, name+'.tgz')\n",
    "       with tarfile.open(tar_path, mode='r:gz') as data:\n",
    "          data.extractall(where)\n",
    "            \n",
    "extract_data('daily-stock','data') # this function call will extract json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "00052-5dba8c02-1524-4ef4-b5f5-85a65e1434c5",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/notebooks/data/daily-stock/aet.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/afl.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/aig.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/al.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/amgn.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/avy.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/b.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/bwa.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/ge.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hal.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hp.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hpq.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/ibm.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/jbl.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/jpm.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/luv.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/met.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/pcg.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/tgt.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/usb.json',\n",
       " '/home/jovyan/notebooks/data/daily-stock/xom.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "\n",
    "here = os.getcwd()\n",
    "datadir = os.path.join(here,'data','daily-stock')\n",
    "filenames = sorted(glob.glob(os.path.join(datadir, '*.json')))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "00053-6816ac9f-f87e-4f04-956c-105517c73273",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'data/daily-stock/*.h5': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%rm data/daily-stock/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "00054-c7da4efd-ed13-469f-a26b-2f6e285603f7",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished : aet.h5\n",
      "Finished : afl.h5\n",
      "Finished : aig.h5\n",
      "Finished : al.h5\n",
      "Finished : amgn.h5\n",
      "Finished : avy.h5\n",
      "Finished : b.h5\n",
      "Finished : bwa.h5\n",
      "Finished : ge.h5\n",
      "Finished : hal.h5\n",
      "Finished : hp.h5\n",
      "Finished : hpq.h5\n",
      "Finished : ibm.h5\n",
      "Finished : jbl.h5\n",
      "Finished : jpm.h5\n",
      "Finished : luv.h5\n",
      "Finished : met.h5\n",
      "Finished : pcg.h5\n",
      "Finished : tgt.h5\n",
      "Finished : usb.h5\n",
      "Finished : xom.h5\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "for fn in filenames:\n",
    "    with open(fn) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')\n",
    "    print(\"Finished : %s\" % out_filename.split(os.path.sep)[-1])\n",
    "\n",
    "filenames = sorted(glob(os.path.join('data', 'daily-stock', '*.h5')))  # data/json/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00055-ee42b11a-a425-4012-a804-10338e33e432",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Sequential code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_id": "00056-114f9f2a-37a4-4414-a5dd-c2a6bf1f3835",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/daily-stock/aet.h5',\n",
       " 'data/daily-stock/afl.h5',\n",
       " 'data/daily-stock/aig.h5',\n",
       " 'data/daily-stock/al.h5',\n",
       " 'data/daily-stock/amgn.h5',\n",
       " 'data/daily-stock/avy.h5',\n",
       " 'data/daily-stock/b.h5',\n",
       " 'data/daily-stock/bwa.h5',\n",
       " 'data/daily-stock/ge.h5',\n",
       " 'data/daily-stock/hal.h5',\n",
       " 'data/daily-stock/hp.h5',\n",
       " 'data/daily-stock/hpq.h5',\n",
       " 'data/daily-stock/ibm.h5',\n",
       " 'data/daily-stock/jbl.h5',\n",
       " 'data/daily-stock/jpm.h5',\n",
       " 'data/daily-stock/luv.h5',\n",
       " 'data/daily-stock/met.h5',\n",
       " 'data/daily-stock/pcg.h5',\n",
       " 'data/daily-stock/tgt.h5',\n",
       " 'data/daily-stock/usb.h5',\n",
       " 'data/daily-stock/xom.h5']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_id": "00057-39e60f20-3194-4f82-8230-9dbef224afa8",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data']\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('data/daily-stock/aet.h5') as hdf:\n",
    "    # This prints a list of all group names:\n",
    "    print(hdf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "00058-957e72f9-0bea-498d-93e2-685cea013bd5",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_hdf('data/daily-stock/aet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "00059-ae84d087-9d8b-4288-bc0d-aad10643fb19",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 286 ms, total: 1.97 s\n",
      "Wall time: 2.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9413176064560878"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)[\"close\"])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00060-32f98a90-1082-4333-bcd8-e6d80f37fea2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercise 9.3\n",
    "\n",
    "Parallelize the code above with Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-029b8755-d3bc-4074-aa42-f6307b99fe82",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- Change the filenames because of the Hadoop environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "00062-06454d0f-52e6-4f2e-abd5-be1ece9f3cbb",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/notebooks/data/daily-stock/aet.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/afl.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/aig.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/al.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/amgn.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/avy.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/b.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/bwa.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/ge.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hal.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hp.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/hpq.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/ibm.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/jbl.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/jpm.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/luv.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/met.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/pcg.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/tgt.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/usb.h5',\n",
       " '/home/jovyan/notebooks/data/daily-stock/xom.h5']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "here = os.getcwd()\n",
    "filenames = sorted(glob.glob(os.path.join(here,'data', 'daily-stock', '*.h5')))\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00063-47d826ff-dcd4-433d-acfa-19d8c3866425",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "If it is not started don't forget the PySpark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00064-03268a38-d5fe-43a3-b5eb-d6e3b081b199",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Computation time is slower because there is a lot of setup, workers creation, there is a lot of communications the correlation function is too small\n",
    "\n",
    "### Exercise 9.4 Fasta file example\n",
    "\n",
    "Use a RDD to calculate the GC content of fasta file nucleotide-sample.txt:\n",
    "\n",
    "$$\\frac{G+C}{A+T+G+C}\\times100 \\% $$\n",
    "\n",
    "Create a rdd from fasta file genome.txt in data directory and count 'G' and 'C' then divide by the total number of bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00065-d670cabd-57f3-478f-8c7c-d5f1f9bbaf0a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Another example\n",
    "\n",
    "Compute the most frequent sequence with 5 bases."
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "53ba4a86-7d9e-483d-acb5-582d7555be2b",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.9",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "source_map": [
   13,
   19,
   26,
   33,
   45,
   54,
   62,
   85,
   105,
   113,
   124,
   134,
   143,
   148,
   153,
   160,
   162,
   166,
   170,
   176,
   183,
   197,
   207,
   210,
   216,
   225,
   228,
   236,
   239,
   251,
   255,
   263,
   267,
   275,
   279,
   287,
   290,
   298,
   301,
   311,
   315,
   323,
   331,
   338,
   341,
   347,
   351,
   392,
   398,
   402,
   407,
   413,
   428,
   439,
   443,
   459,
   463,
   467,
   473,
   477,
   493,
   499,
   503,
   509,
   513,
   525
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
