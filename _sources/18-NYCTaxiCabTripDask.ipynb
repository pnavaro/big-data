{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-527bc450-25b5-48e1-bf73-857c366f5001",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Dask dataframes on HDFS\n",
    "\n",
    "To use Dask dataframes in parallel across an HDFS cluster to read CSV data. We can coordinate these computations with [distributed](http://distributed.dask.org/en/latest/) and dask.dataframe.\n",
    "\n",
    "As Spark, Dask can work in cluster mode. There is several ways to launch a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-8a5ef57b-e4fc-45af-aa30-8389de70bc3c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-a625f38b-4c20-409c-b827-196021dd4c47",
    "deepnote_cell_type": "code",
    "execution_millis": 1048,
    "execution_start": 1606918412381,
    "output_cleared": false,
    "source_hash": "4e55e2c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-9246761f-912e-4a50-8f8e-e6f1bad38de0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Remote clusters via SSH\n",
    "\n",
    "Code below can be used to launch a Dask SSH cluster on svmass2 server. \n",
    "\n",
    "```python\n",
    "from dask.distributed import SSHCluster\n",
    "\n",
    "svpes = [f\"svpe{i:1d}\" for i in range(1,10)]\n",
    "print(svpes)\n",
    "cluster = SSHCluster([\"localhost\"] + svpes)\n",
    "cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-82aab717-71ff-46e7-92ff-2b5c44193ab6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Yarn cluster\n",
    "\n",
    "Follow these [instructions](https://yarn.dask.org/en/latest/environments.html) to create the environment file.\n",
    "\n",
    "```\n",
    "from dask_yarn import YarnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a cluster where each worker has two cores and eight GiB of memory\n",
    "cluster = YarnCluster(environment='environment.tar.gz',\n",
    "                      worker_vcores=2,\n",
    "                      worker_memory=\"8GiB\")\n",
    "# Scale out to ten such workers\n",
    "cluster.scale(10)\n",
    "\n",
    "# Connect to the cluster\n",
    "client = Client(cluster)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-c4c24411-83e5-48f3-9a5e-8bd403731783",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## SLURM Cluster\n",
    "\n",
    "You can use the dask module [dask_jobqueue](https://jobqueue.dask.org/en/latest/) \n",
    "to launch a Dask cluster with the job manager SLURM.\n",
    "\n",
    "```py\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(cores=16,\n",
    "             queue='test',\n",
    "             project='myproject',\n",
    "             memory=\"16GB\",\n",
    "             walltime=\"01:00:00\")\n",
    "```\n",
    "\n",
    "The cluster generates a traditional job script and submits that an appropriate number of times to the job queue. You can see the job script that it will generate as follows:\n",
    "\n",
    "```py\n",
    "print(cluster.job_script())\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "#SBATCH -J dask-worker\n",
    "#SBATCH -p test\n",
    "#SBATCH -A myproject\n",
    "#SBATCH -n 1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=15G\n",
    "#SBATCH -t 01:00:00\n",
    "\n",
    "/opt/tljh/user/envs/big-data/bin/python -m distributed.cli.dask_worker tcp://192.168.2.54:40623 --nthreads 4 --nprocs 4 --memory-limit 4.00GB --name name --nanny --death-timeout 60\n",
    "```\n",
    "Use the script above to submit your dask pipeline to the HPC server of your insttitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-2b67d07a-67bd-4c56-9869-96681e67b32e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## New York City Taxi Cab Trip\n",
    "\n",
    "We look at [the New York City Taxi Cab dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). This includes every ride made in the city of New York since 2009.\n",
    "\n",
    "On [this website](http://chriswhong.github.io/nyctaxi/) you can see the data for one random NYC yellow taxi on a single day.\n",
    "\n",
    "On [this post](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/), you can see an analysis of this dataset. Postgres and R scripts are available on [GitHub](https://github.com/toddwschneider/nyc-taxi-data). There is also a dashboard available [here](https://toddwschneider.com/dashboards/nyc-taxi-ridehailing-uber-lyft-data/) that updates monthly with the latest taxi, Uber, and Lyft aggregate stats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-580f4583-2389-4de5-926a-3e4190b8585b",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "```python\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n",
    "\n",
    "`nyc2014` is a dask.dataframe objects which present a subset of the Pandas API to the user, but farm out all of the work to the many Pandas dataframes they control across the network.\n",
    "\n",
    "nyc2014 = dd.read_csv('/opt/datasets/nyc-data/2014/yellow*.csv',\n",
    "parse_dates=['pickup_datetime', 'dropoff_datetime'],\n",
    "skipinitialspace=True)\n",
    "nyc2014 = c.persist(nyc2014)\n",
    "progress(nyc2014)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-61cbcc92-4356-437b-adac-ea5609c5df9d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Exercises \n",
    "\n",
    "- Display head of the dataframe\n",
    "- Display number of rows of this dataframe.\n",
    "- Compute the total number of passengers.\n",
    "- Count occurrences in the payment_type column both for the full dataset, and filtered by zero tip (tip_amount == 0).\n",
    "- Create a new column, tip_fraction\n",
    "- Plot the average of the new column tip_fraction grouped by day of week.\n",
    "- Plot the average of the new column tip_fraction grouped by hour of day.\n",
    "\n",
    "[Dask dataframe documentation](http://docs.dask.org/en/latest/dataframe.html)"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "379fcb20-a410-4e6b-9597-3e0eecd8481f",
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
