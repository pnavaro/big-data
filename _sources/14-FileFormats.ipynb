{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-d7da12dc-75b8-418b-a299-239e69c86a15",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# File Formats\n",
    "\n",
    "I present three data formats, feather, parquet and hdf but it exists several more like [Apache Avro](http://avro.apache.org/docs/current/) or [Apache ORC](https://orc.apache.org). \n",
    "\n",
    "These data formats may be more appropriate in certain situations. \n",
    "However, the software needed to handle them is either more difficult \n",
    "to install, incomplete, or more difficult to use because less \n",
    "documentation is provided. For ORC and AVRO the python libraries \n",
    "offered are less well maintained than the formats we will see. You can find many on \n",
    "the web but it is hard to know which one is the most stable. \n",
    "- [pyorc](https://github.com/noirello/pyorc)\n",
    "- [avro](https://avro.apache.org/docs/1.10.0/gettingstartedpython.html) and [fastavro](https://github.com/fastavro/fastavro)\n",
    "The following formats are supported\n",
    "by pandas and apache arrow. These softwares are supported by very strong communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-f391c3d6-f2be-40ff-935a-7dbe66500534",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Feather\n",
    "\n",
    "For light data, it is recommanded to use [Feather](https://github.com/wesm/feather). It is a fast, interoperable data frame storage that comes with bindings for python and R.\n",
    "\n",
    "Feather uses also the Apache Arrow columnar memory specification to represent binary data on disk. This makes read and write operations very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-668ae76d-ef35-44ea-9e89-ed68896e97a1",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Parquet file format\n",
    "\n",
    "[Parquet format](https://github.com/apache/parquet-format) is a common binary data store, used particularly in the Hadoop/big-data sphere. It provides several advantages relevant to big-data processing:\n",
    "\n",
    "The Apache Parquet project provides a standardized open-source columnar storage format for use in data analysis systems. It was created originally for use in Apache Hadoop with systems like Apache Drill, Apache Hive, Apache Impala, and Apache Spark adopting it as a shared standard for high performance data IO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-20ddaff2-d1b2-4377-bba4-8dbe36cb4b0e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Hierarchical Data Format\n",
    "\n",
    " [HDF](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) is a self-describing data format\n",
    "allowing an application to interpret the structure and \n",
    "contents of a file with no outside information. \n",
    "One HDF file can hold a mix of related objects \n",
    "which can be accessed as a group or as individual objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-f27ca6ee-3572-4f04-9682-a21cb054c956",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Let's create some big dataframe with consitent data (Floats) and 10% of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-86f06cb5-60f9-4b76-8b28-d8be6216d31c",
    "deepnote_cell_type": "code",
    "execution_millis": 365,
    "execution_start": 1605607212760,
    "output_cleared": false,
    "source_hash": "2148f3e9"
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "arr = np.random.randn(500000) # 10% nulls\n",
    "arr[::10] = np.nan\n",
    "df = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-6c3102ae-966b-4c76-863d-02e663356c24",
    "deepnote_cell_type": "code",
    "execution_millis": 9969,
    "execution_start": 1605607226298,
    "output_cleared": false,
    "source_hash": "601b1750",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-eebd2468-f5fd-4c69-83c1-1c7680da818a",
    "deepnote_cell_type": "code",
    "execution_millis": 904,
    "execution_start": 1605607570330,
    "output_cleared": false,
    "source_hash": "648965ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%rm test.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-3efe272e-e2c6-4f92-a559-d1a7e7cc422e",
    "deepnote_cell_type": "code",
    "execution_millis": 3074,
    "execution_start": 1605607572677,
    "output_cleared": false,
    "source_hash": "9511287b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_hdf(\"test.h5\", key=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-e32d9d18-f251-4149-a739-083b7e98905c",
    "deepnote_cell_type": "code",
    "execution_millis": 1258,
    "execution_start": 1605607364857,
    "output_cleared": false,
    "source_hash": "3658418d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-3d58cd78-c520-45a8-aee0-c9786b4ee486",
    "deepnote_cell_type": "code",
    "execution_millis": 524,
    "execution_start": 1605607390362,
    "output_cleared": false,
    "source_hash": "8a260d4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_feather('test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-ea9189a2-035b-4a8e-84cc-1c71a317ef88",
    "deepnote_cell_type": "code",
    "execution_millis": 51,
    "execution_start": 1605607453945,
    "output_cleared": false,
    "source_hash": "48eefa13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "du -sh test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-48514262-1c39-4f50-8244-d1e3f75760d8",
    "deepnote_cell_type": "code",
    "execution_millis": 2122,
    "execution_start": 1605607494359,
    "output_cleared": false,
    "source_hash": "5e7e15ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-fb4d1486-9047-47fd-804b-d8498e9234ce",
    "deepnote_cell_type": "code",
    "execution_millis": 1400,
    "execution_start": 1605607580459,
    "output_cleared": false,
    "source_hash": "71468c8a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_hdf(\"test.h5\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-34278fa4-4bee-4496-a023-b02519d5e310",
    "deepnote_cell_type": "code",
    "execution_millis": 871,
    "execution_start": 1605607590036,
    "output_cleared": false,
    "source_hash": "afb15a98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-304f31cd-09e7-4629-8bcd-29d0d3de97a8",
    "deepnote_cell_type": "code",
    "execution_millis": 504,
    "execution_start": 1605607599396,
    "output_cleared": false,
    "source_hash": "5110928f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_feather(\"test.feather\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-200d2d6f-7b4f-4739-87e5-ee1d535d3eea",
    "deepnote_cell_type": "code",
    "execution_millis": 0,
    "execution_start": 1605607647672,
    "output_cleared": false,
    "source_hash": "83688577",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we create a new big dataframe with a column of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-9d559345-404e-4d73-bbe0-ab773d5e751b",
    "deepnote_cell_type": "code",
    "execution_millis": 29715,
    "execution_start": 1605607754053,
    "output_cleared": false,
    "source_hash": "11d4259f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lorem import sentence\n",
    "\n",
    "words = np.array(sentence().strip().lower().replace(\".\", \" \").split())\n",
    "\n",
    "# Set the seed so that the numbers can be reproduced.\n",
    "np.random.seed(0)  \n",
    "n = 1000000\n",
    "df = pd.DataFrame(np.c_[np.random.randn(n, 5),\n",
    "                  np.random.randint(0,10,(n, 2)),\n",
    "                  np.random.randint(0,1,(n, 2)),\n",
    "np.array([np.random.choice(words) for i in range(n)])] , \n",
    "columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "df[\"A\"][::10] = np.nan\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-5a1ffebb-59dd-4e23-ad74-420bf1883fe2",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-dbb81bcb-1dae-4e78-b88f-7261409341b8",
    "deepnote_cell_type": "code",
    "execution_millis": 6742,
    "execution_start": 1605607804335,
    "output_cleared": false,
    "source_hash": "86969307",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-4af8fb60-2e45-41a8-8173-c03959c65112",
    "deepnote_cell_type": "code",
    "execution_millis": 10808,
    "execution_start": 1605607849621,
    "output_cleared": false,
    "source_hash": "60cee7f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_hdf('test.h5', key=\"test\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-169983f0-f35d-4d2d-a62d-dac421509f03",
    "deepnote_cell_type": "code",
    "execution_millis": 2654,
    "execution_start": 1605607878875,
    "output_cleared": false,
    "source_hash": "a5d7fbf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_feather('test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-a8112e1b-2c08-47ac-b740-059f6cb840b2",
    "deepnote_cell_type": "code",
    "execution_millis": 2305,
    "execution_start": 1605607913541,
    "output_cleared": false,
    "source_hash": "ad143313",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-2b95458f-c287-4419-86e5-c617a63ef1ef",
    "deepnote_cell_type": "code",
    "execution_millis": 8728,
    "execution_start": 1605607923868,
    "output_cleared": false,
    "source_hash": "bdf7d1af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_csv(\"test.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-ee4b525b-cc49-49a1-85c0-c79bbb36d662",
    "deepnote_cell_type": "code",
    "execution_millis": 13601,
    "execution_start": 1605607936800,
    "output_cleared": false,
    "source_hash": "771dceb3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_hdf(\"test.h5\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-7e533430-03e7-4486-9512-4a5e3fdc7fca",
    "deepnote_cell_type": "code",
    "execution_millis": 9103,
    "execution_start": 1605607954950,
    "output_cleared": false,
    "source_hash": "a1e5a992",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_feather('test.feather')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-c284124d-89af-4ee1-9c47-60d6d5a0241c",
    "deepnote_cell_type": "code",
    "execution_millis": 10083,
    "execution_start": 1605607999155,
    "output_cleared": false,
    "source_hash": "f37bbfe5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_parquet('test.parquet')\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-c22ed092-a58d-4375-affd-da5887c25b6d",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1605608063341,
    "output_cleared": false,
    "source_hash": "f30f989a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-89b1fca0-a882-4bf4-a3cb-7a8842bf91b6",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-8618020c-4639-4a88-b9f2-0e1212e9c7cf",
    "deepnote_cell_type": "code",
    "execution_millis": 470,
    "execution_start": 1605608091018,
    "output_cleared": false,
    "source_hash": "d56ddf52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['J'] = pd.Categorical(df.J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-cf2defbc-03c8-4b43-becd-82bc3e39cdee",
    "deepnote_cell_type": "code",
    "execution_millis": 1959,
    "execution_start": 1605608105636,
    "output_cleared": false,
    "source_hash": "1ddaec9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_feather('test.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-8c3f3bc6-d8b5-4915-a22e-89d77df8f8a3",
    "deepnote_cell_type": "code",
    "execution_millis": 2699,
    "execution_start": 1605608134600,
    "output_cleared": false,
    "source_hash": "3658418d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time df.to_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-7c986385-4bc6-4c82-8079-36a53df467ef",
    "deepnote_cell_type": "code",
    "execution_millis": 6995,
    "execution_start": 1605608145002,
    "output_cleared": false,
    "source_hash": "a1e5a992",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_feather('test.feather')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-245a1c05-2399-4d6c-95f0-195be6f23e3e",
    "deepnote_cell_type": "code",
    "execution_millis": 10342,
    "execution_start": 1605608190354,
    "output_cleared": false,
    "source_hash": "f37bbfe5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_parquet('test.parquet')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-832b0847-4adb-4f4b-8357-66febb207dda",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Feather or Parquet\n",
    "\n",
    "- Parquet format is designed for long-term storage, where Arrow is more intended for short term or ephemeral storage because files volume are larger.\n",
    "- Parquet is usually more expensive to write than Feather as it features more layers of encoding and compression. \n",
    "- Feather is unmodified raw columnar Arrow memory. We will probably add simple compression to Feather in the future.\n",
    "- Due to dictionary encoding, RLE encoding, and data page compression, Parquet files will often be much smaller than Feather files\n",
    "- Parquet is a standard storage format for analytics that's supported by Spark. So if you are doing analytics, Parquet is a good option as a reference storage format for query by multiple systems\n",
    "\n",
    "[source stackoverflow](https://stackoverflow.com/questions/48083405/what-are-the-differences-between-feather-and-parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-9a75a293-b946-4598-bd67-3fd8415c48c2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Apache Arrow\n",
    "\n",
    "[Arrow](https://arrow.apache.org/docs/python/) is a columnar in-memory analytics layer designed to accelerate big data. \n",
    "It houses a set of canonical in-memory representations of \n",
    "hierarchical data along with multiple language-bindings \n",
    "for structure manipulation. Arrow offers an unified way to be able to \n",
    "share the same data representation among languages and it will certainly be \n",
    "the next standard to store dataframes in all languages.\n",
    "\n",
    "- [R package](https://cran.r-project.org/web/packages/arrow/index.html)\n",
    "- [Julia package](https://github.com/JuliaData/Arrow.jl)\n",
    "- [GitHub project](https://github.com/apache/arrow)\n",
    "\n",
    "![](images/arrow_ecosystem.png)\n",
    "\n",
    "Apache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. [PyArrow](https://arrow.apache.org/docs/python/) includes Python bindings to read and write Parquet files with pandas.\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure\n",
    "\n",
    "![arrow](images/arrow.png)\n",
    "\n",
    "- https://arrow.apache.org/docs/python/csv.html\n",
    "- https://arrow.apache.org/docs/python/feather.html\n",
    "- https://arrow.apache.org/docs/python/parquet.html\n",
    "\n",
    "\n",
    "Example:\n",
    "```py\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "arr = np.random.randn(500000) # 10% nulls\n",
    "arr[::10] = np.nan\n",
    "df = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})\n",
    "\n",
    "hdfs = pa.hdfs.connect()\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_to_dataset(table, root_path=\"test\", filesystem=hdfs)\n",
    "hdfs.ls(\"test\")\n",
    "\n",
    "```\n",
    "### Read CSV from HDFS\n",
    "\n",
    "Put the file test.csv on hdfs system \n",
    "\n",
    "```python\n",
    "from pyarrow import csv\n",
    "with hdfs.open(\"/data/nycflights/1999.csv\", \"rb\") as f:\n",
    " df = pd.read_csv(f, nrows = 10)\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### Read Parquet File from HDFS with pandas\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "wikipedia = pd.read_parquet(\"hdfs://svmass2.mass.uhb.fr:54310/data/pagecounts-parquet/part-00007-8575060f-6b57-45ea-bf1d-cd77b6141f70.snappy.parquet\", engine=’pyarrow’)\n",
    "print(wikipedia.head())\n",
    "```\n",
    "### Read Parquet File with pyarrow\n",
    "\n",
    "```py\n",
    "table = pq.read_table(\"example.parquet\")\n",
    "```\n",
    "\n",
    "### Writing a parquet file from Apache Arrow\n",
    "```py\n",
    "pq.write_table(table, \"example.parquet\")\n",
    "```\n",
    "\n",
    "### Check metadata\n",
    "```py\n",
    "parquet_file = pq.ParquetFile(\"example.parquet\")\n",
    "print(parquet_file.metadata)\n",
    "```\n",
    "\n",
    "### See schema\n",
    "```py\n",
    "print(parquet_file.schema)\n",
    "```\n",
    "\n",
    "### Connect to the Hadoop file system\n",
    "\n",
    "```py\n",
    "hdfs = pa.hdfs.connect()\n",
    "\n",
    "# copy to local\n",
    "with hdfs.open(\"user.txt\", \"rb\") as f:\n",
    "    f.download(\"user.text\")\n",
    "\n",
    "# write parquet file on hdfs\n",
    "with open(\"example.parquet\", \"rb\") as f:\n",
    "    pa.HadoopFileSystem.upload(hdfs, \"example.parquet\", f)\n",
    "\n",
    "# List files\n",
    "for f in hdfs.ls(\"/user/navaro_p\"):\n",
    "    print(f)\n",
    "\n",
    "# create a small dataframe and write it to hadoop file system\n",
    "small_df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
    "table = pa.Table.from_pandas(small_df)\n",
    "pq.write_table(table, \"small_df.parquet\", filesystem=hdfs)\n",
    "\n",
    "\n",
    "# Read files from Hadoop with pandas\n",
    "with hdfs.open(\"/data/irmar.csv\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Read parquet file from Hadoop with pandas\n",
    "server = \"hdfs://svmass2.mass.uhb.fr:54310\"\n",
    "path = \"data/pagecounts-parquet/part-00007-8575060f-6b57-45ea-bf1d-cd77b6141f70.snappy.parquet\"\n",
    "pagecount = pd.read_parquet(os.path.join(server, path), engine=\"pyarrow\")\n",
    "print(pagecount.head())\n",
    "\n",
    "# Read parquet file from Hadoop with pyarrow\n",
    "table = pq.read_table(os.path.join(server,path))\n",
    "print(table.schema)\n",
    "df = table.to_pandas()\n",
    "print(df.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-c0409d84-12d4-42c5-adb2-ce319b58d5be",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Exercise\n",
    "\n",
    "- Take the second dataframe with string as last column\n",
    "- Create an arrow table from pandas dataframe\n",
    "- Write the file test.parquet from arrow table\n",
    "- Print metadata from this parquet file\n",
    "- Print schema\n",
    "- Upload the file to hadoop file system\n",
    "- Read this file from hadoop file system and print dataframe head\n",
    "\n",
    "\n",
    "Hint: check the doc https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-3784ae35-78ff-46a4-ab34-c9c075b05220",
    "deepnote_cell_type": "code",
    "execution_millis": 1093,
    "execution_start": 1605608783188,
    "output_cleared": false,
    "source_hash": "313d4d07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "arr = np.random.randn(500000) # 10% nulls\n",
    "arr[::10] = np.nan\n",
    "df = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})\n",
    "\n",
    "# hdfs = pa.hdfs.connect()\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_to_dataset(table, root_path=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00038-1f977a10-b43e-45c5-b7d2-163bc2d57241",
    "deepnote_cell_type": "code",
    "execution_millis": 147,
    "execution_start": 1605608797709,
    "output_cleared": false,
    "source_hash": "cd45f8b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00039-fb75b3da-5a38-4c7f-bc3c-13107a7be153",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```python\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lorem import sentence\n",
    "from time import time\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "1 Creation de la dataframe avec des chaines en derniere colonne\n",
    "\"\"\")\n",
    "\n",
    "words = np.array(sentence().strip().lower().replace(\".\", \" \").split())\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 1000000\n",
    "df = pd.DataFrame(np.c_[np.random.randn(n, 5),\n",
    "                  np.random.randint(0,10,(n, 2)),\n",
    "                  np.random.randint(0,1,(n, 2)),\n",
    "np.array([np.random.choice(words) for i in range(n)])] ,\n",
    "columns=list('ABCDEFGHIJ'))\n",
    "\n",
    "df[\"A\"][::10] = np.nan\n",
    "print(len(df))\n",
    "\n",
    "print(\"\"\"\n",
    "2 Creation de la table Arrow\n",
    "\"\"\")\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "print(\"\"\"\n",
    "2 Creation du fichier test.parquet depuis la table arrow\n",
    "\"\"\")\n",
    "\n",
    "pq.write_table(table, \"test.parquet\")\n",
    "\n",
    "print(\"\"\"\n",
    "3 Visualiser les metadata\n",
    "\"\"\")\n",
    "\n",
    "parquet_file = pq.ParquetFile(\"test.parquet\")\n",
    "print(parquet_file.metadata)\n",
    "print(\" Autre maniere de faire \")\n",
    "print(pq.read_metadata(\"test.parquet\"))\n",
    "\n",
    "print(\"\"\"\n",
    "4 Afficher le schema\n",
    "\"\"\")\n",
    "\n",
    "print(parquet_file.schema)\n",
    "\n",
    "print(\"\"\"\n",
    "5 copier le ficher parquet sur le systeme hadoop (atention c'est long)\n",
    "\"\"\")\n",
    "\n",
    "hdfs = pa.hdfs.connect()\n",
    "\n",
    "with open(\"test.parquet\", \"rb\") as f:\n",
    "    pa.HadoopFileSystem.upload(hdfs, \"test.parquet\", f)\n",
    "\n",
    "for f in hdfs.ls(\"/user/navaro_p\"):\n",
    "    print(f)\n",
    "server = \"hdfs://svmass2.mass.uhb.fr:54310\"\n",
    "path = \"user/navaro_p/test.parquet\"\n",
    "table = pq.read_table(os.path.join(server,path))\n",
    "print(table.schema)\n",
    "df = table.to_pandas()\n",
    "print(df.head())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-79f4010c-254b-494c-baca-abd058d7cb85",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-d6f4c0c3-618a-4d03-9e0a-d1310f5553eb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a7088f08-45cf-4fa4-910d-a13ff12600d1",
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
