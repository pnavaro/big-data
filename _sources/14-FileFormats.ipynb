{"cells":[{"cell_type":"markdown","source":"# File Formats\n\nI present three data formats, feather, parquet and hdf but it exists several more like [Apache Avro](http://avro.apache.org/docs/current/) or [Apache ORC](https://orc.apache.org). \n\nThese data formats may be more appropriate in certain situations. \nHowever, the software needed to handle them is either more difficult \nto install, incomplete, or more difficult to use because less \ndocumentation is provided. For ORC and AVRO the python libraries \noffered are less well maintained than the formats we will see. You can find many on \nthe web but it is hard to know which one is the most stable. \n- [pyorc](https://github.com/noirello/pyorc)\n- [avro](https://avro.apache.org/docs/1.10.0/gettingstartedpython.html) and [fastavro](https://github.com/fastavro/fastavro)\nThe following formats are supported\nby pandas and apache arrow. These softwares are supported by very strong communities.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00000-d7da12dc-75b8-418b-a299-239e69c86a15"}},{"cell_type":"markdown","source":"## Feather\n\nFor light data, it is recommanded to use [Feather](https://github.com/wesm/feather). It is a fast, interoperable data frame storage that comes with bindings for python and R.\n\nFeather uses also the Apache Arrow columnar memory specification to represent binary data on disk. This makes read and write operations very fast.","metadata":{"deepnote_cell_type":"markdown","cell_id":"00001-f391c3d6-f2be-40ff-935a-7dbe66500534"}},{"cell_type":"markdown","source":"## Parquet file format\n\n[Parquet format](https://github.com/apache/parquet-format) is a common binary data store, used particularly in the Hadoop/big-data sphere. It provides several advantages relevant to big-data processing:\n\nThe Apache Parquet project provides a standardized open-source columnar storage format for use in data analysis systems. It was created originally for use in Apache Hadoop with systems like Apache Drill, Apache Hive, Apache Impala, and Apache Spark adopting it as a shared standard for high performance data IO.\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-668ae76d-ef35-44ea-9e89-ed68896e97a1"}},{"cell_type":"markdown","source":"## Hierarchical Data Format\n\n [HDF](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) is a self-describing data format\nallowing an application to interpret the structure and \ncontents of a file with no outside information. \nOne HDF file can hold a mix of related objects \nwhich can be accessed as a group or as individual objects. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-20ddaff2-d1b2-4377-bba4-8dbe36cb4b0e"}},{"cell_type":"markdown","source":"Let's create some big dataframe with consitent data (Floats) and 10% of missing values:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-f27ca6ee-3572-4f04-9682-a21cb054c956"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00002-86f06cb5-60f9-4b76-8b28-d8be6216d31c","output_cleared":false,"source_hash":"2148f3e9","execution_millis":365,"execution_start":1605607212760},"source":"import feather\nimport pandas as pd\nimport numpy as np\narr = np.random.randn(500000) # 10% nulls\narr[::10] = np.nan\ndf = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-6c3102ae-966b-4c76-863d-02e663356c24","output_cleared":false,"source_hash":"601b1750","execution_millis":9969,"execution_start":1605607226298},"source":"%time df.to_csv('test.csv')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 9.31 s, sys: 378 ms, total: 9.69 s\nWall time: 9.93 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-eebd2468-f5fd-4c69-83c1-1c7680da818a","output_cleared":false,"source_hash":"648965ca","execution_start":1605607570330,"execution_millis":904},"source":"%rm test.h5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-3efe272e-e2c6-4f92-a559-d1a7e7cc422e","output_cleared":false,"source_hash":"9511287b","execution_millis":3074,"execution_start":1605607572677},"source":"%time df.to_hdf(\"test.h5\", key=\"test\")","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 377 ms, sys: 2.15 s, total: 2.53 s\nWall time: 3.08 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-e32d9d18-f251-4149-a739-083b7e98905c","output_cleared":false,"source_hash":"3658418d","execution_millis":1258,"execution_start":1605607364857},"source":"%time df.to_parquet('test.parquet')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 615 ms, sys: 373 ms, total: 987 ms\nWall time: 1.3 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-3d58cd78-c520-45a8-aee0-c9786b4ee486","output_cleared":false,"source_hash":"8a260d4f","execution_millis":524,"execution_start":1605607390362},"source":"%time df.to_feather('test.feather')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 321 ms, sys: 180 ms, total: 502 ms\nWall time: 564 ms\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-ea9189a2-035b-4a8e-84cc-1c71a317ef88","output_cleared":false,"source_hash":"48eefa13","execution_millis":51,"execution_start":1605607453945},"source":"%%bash\ndu -sh test.*","execution_count":null,"outputs":[{"name":"stdout","text":"88M\ttest.csv\n36M\ttest.feather\n205M\ttest.h5\n38M\ttest.parquet\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00008-48514262-1c39-4f50-8244-d1e3f75760d8","output_cleared":false,"source_hash":"5e7e15ac","execution_millis":2122,"execution_start":1605607494359},"source":"%%time\ndf = pd.read_csv(\"test.csv\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 1.32 s, sys: 829 ms, total: 2.15 s\nWall time: 2.16 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"500000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00009-fb4d1486-9047-47fd-804b-d8498e9234ce","output_cleared":false,"source_hash":"71468c8a","execution_millis":1400,"execution_start":1605607580459},"source":"%%time\ndf = pd.read_hdf(\"test.h5\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 337 ms, sys: 998 ms, total: 1.34 s\nWall time: 1.39 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"500000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-34278fa4-4bee-4496-a023-b02519d5e310","output_cleared":false,"source_hash":"afb15a98","execution_millis":871,"execution_start":1605607590036},"source":"%%time\ndf = pd.read_parquet(\"test.parquet\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 373 ms, sys: 737 ms, total: 1.11 s\nWall time: 900 ms\n","output_type":"stream"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"500000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","cell_id":"00003-304f31cd-09e7-4629-8bcd-29d0d3de97a8","output_cleared":false,"source_hash":"5110928f","execution_millis":504,"execution_start":1605607599396},"source":"%%time\ndf = pd.read_feather(\"test.feather\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 164 ms, sys: 579 ms, total: 742 ms\nWall time: 486 ms\n","output_type":"stream"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"500000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-200d2d6f-7b4f-4739-87e5-ee1d535d3eea","output_cleared":false,"source_hash":"83688577","execution_millis":0,"execution_start":1605607647672},"source":"# Now we create a new big dataframe with a column of strings","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-9d559345-404e-4d73-bbe0-ab773d5e751b","output_cleared":false,"source_hash":"11d4259f","execution_millis":29715,"execution_start":1605607754053},"source":"import numpy as np\nimport pandas as pd\nfrom lorem import sentence\n\nwords = np.array(sentence().strip().lower().replace(\".\", \" \").split())\n\n# Set the seed so that the numbers can be reproduced.\nnp.random.seed(0)  \nn = 1000000\ndf = pd.DataFrame(np.c_[np.random.randn(n, 5),\n                  np.random.randint(0,10,(n, 2)),\n                  np.random.randint(0,1,(n, 2)),\nnp.array([np.random.choice(words) for i in range(n)])] , \ncolumns=list('ABCDEFGHIJ'))\n\ndf[\"A\"][::10] = np.nan\nlen(df)\n","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00018-5a1ffebb-59dd-4e23-ad74-420bf1883fe2"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-dbb81bcb-1dae-4e78-b88f-7261409341b8","output_cleared":false,"source_hash":"86969307","execution_millis":6742,"execution_start":1605607804335},"source":"%%time\ndf.to_csv('test.csv', index=False)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 5.44 s, sys: 1.03 s, total: 6.48 s\nWall time: 6.78 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00015-4af8fb60-2e45-41a8-8173-c03959c65112","output_cleared":false,"source_hash":"60cee7f2","execution_millis":10808,"execution_start":1605607849621},"source":"%%time\ndf.to_hdf('test.h5', key=\"test\", mode=\"w\")","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/pandas/core/generic.py:2505: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], dtype='object')]\n\n  encoding=encoding,\nCPU times: user 5.57 s, sys: 4.7 s, total: 10.3 s\nWall time: 10.8 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-169983f0-f35d-4d2d-a62d-dac421509f03","output_cleared":false,"source_hash":"a5d7fbf1","execution_millis":2654,"execution_start":1605607878875},"source":"%%time\ndf.to_feather('test.feather')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 2.02 s, sys: 1.61 s, total: 3.64 s\nWall time: 3.66 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00017-a8112e1b-2c08-47ac-b740-059f6cb840b2","output_cleared":false,"source_hash":"ad143313","execution_millis":2305,"execution_start":1605607913541},"source":"%%time\ndf.to_parquet('test.parquet')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 2.63 s, sys: 1.35 s, total: 3.98 s\nWall time: 4.29 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00018-2b95458f-c287-4419-86e5-c617a63ef1ef","output_cleared":false,"source_hash":"bdf7d1af","execution_millis":8728,"execution_start":1605607923868},"source":"%%time \ndf = pd.read_csv(\"test.csv\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 2.54 s, sys: 6.22 s, total: 8.77 s\nWall time: 8.77 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00019-ee4b525b-cc49-49a1-85c0-c79bbb36d662","output_cleared":false,"source_hash":"771dceb3","execution_millis":13601,"execution_start":1605607936800},"source":"%%time \ndf = pd.read_hdf(\"test.h5\")\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 4.26 s, sys: 9.34 s, total: 13.6 s\nWall time: 13.6 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00020-7e533430-03e7-4486-9512-4a5e3fdc7fca","output_cleared":false,"source_hash":"a1e5a992","execution_millis":9103,"execution_start":1605607954950},"source":"%%time \ndf = pd.read_feather('test.feather')\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 3.88 s, sys: 5.99 s, total: 9.87 s\nWall time: 9.1 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-c284124d-89af-4ee1-9c47-60d6d5a0241c","output_cleared":false,"source_hash":"f37bbfe5","execution_millis":10083,"execution_start":1605607999155},"source":"%%time \ndf = pd.read_parquet('test.parquet')\nlen(df)\n","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 4.57 s, sys: 6.87 s, total: 11.4 s\nWall time: 10.1 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00027-c22ed092-a58d-4375-affd-da5887c25b6d","output_cleared":false,"source_hash":"f30f989a","execution_millis":5,"execution_start":1605608063341},"source":"df.head(10)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":10,"columns":[{"name":"A","dtype":"object","stats":{"unique_count":9,"nan_count":1,"categories":[{"name":"-0.977277879876411","count":1},{"name":"8 others","count":8},{"name":"Missing","count":1}]}},{"name":"B","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"0.4001572083672233","count":1},{"name":"0.9500884175255894","count":1},{"name":"8 others","count":8}]}},{"name":"C","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"0.9787379841057392","count":1},{"name":"-0.1513572082976979","count":1},{"name":"8 others","count":8}]}},{"name":"D","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"2.240893199201458","count":1},{"name":"-0.10321885179355784","count":1},{"name":"8 others","count":8}]}},{"name":"E","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"1.8675579901499675","count":1},{"name":"0.41059850193837233","count":1},{"name":"8 others","count":8}]}},{"name":"F","dtype":"object","stats":{"unique_count":6,"nan_count":0,"categories":[{"name":"6","count":3},{"name":"0","count":2},{"name":"4 others","count":5}]}},{"name":"G","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"5","count":4},{"name":"0","count":3},{"name":"3 others","count":3}]}},{"name":"H","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"0","count":10}]}},{"name":"I","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"0","count":10}]}},{"name":"J","dtype":"object","stats":{"unique_count":6,"nan_count":0,"categories":[{"name":"quisquam","count":2},{"name":"modi","count":2},{"name":"4 others","count":6}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"A":"nan","B":"0.4001572083672233","C":"0.9787379841057392","D":"2.240893199201458","E":"1.8675579901499675","F":"0","G":"4","H":"0","I":"0","J":"quisquam","_deepnote_index_column":0},{"A":"-0.977277879876411","B":"0.9500884175255894","C":"-0.1513572082976979","D":"-0.10321885179355784","E":"0.41059850193837233","F":"5","G":"5","H":"0","I":"0","J":"quisquam","_deepnote_index_column":1},{"A":"0.144043571160878","B":"1.454273506962975","C":"0.7610377251469934","D":"0.12167501649282841","E":"0.44386323274542566","F":"6","G":"1","H":"0","I":"0","J":"modi","_deepnote_index_column":2},{"A":"0.33367432737426683","B":"1.4940790731576061","C":"-0.20515826376580087","D":"0.31306770165090136","E":"-0.8540957393017248","F":"0","G":"5","H":"0","I":"0","J":"eius","_deepnote_index_column":3},{"A":"-2.5529898158340787","B":"0.6536185954403606","C":"0.8644361988595057","D":"-0.7421650204064419","E":"2.2697546239876076","F":"6","G":"7","H":"0","I":"0","J":"magnam","_deepnote_index_column":4},{"A":"-1.4543656745987648","B":"0.04575851730144607","C":"-0.1871838500258336","D":"1.5327792143584575","E":"1.469358769900285","F":"6","G":"0","H":"0","I":"0","J":"velit","_deepnote_index_column":5},{"A":"0.1549474256969163","B":"0.37816251960217356","C":"-0.8877857476301128","D":"-1.980796468223927","E":"-0.3479121493261526","F":"8","G":"0","H":"0","I":"0","J":"magnam","_deepnote_index_column":6},{"A":"0.15634896910398005","B":"1.2302906807277207","C":"1.2023798487844113","D":"-0.3873268174079523","E":"-0.30230275057533557","F":"5","G":"5","H":"0","I":"0","J":"modi","_deepnote_index_column":7},{"A":"-1.0485529650670926","B":"-1.4200179371789752","C":"-1.7062701906250126","D":"1.9507753952317897","E":"-0.5096521817516535","F":"7","G":"5","H":"0","I":"0","J":"velit","_deepnote_index_column":8},{"A":"-0.4380743016111864","B":"-1.2527953600499262","C":"0.7774903558319101","D":"-1.6138978475579515","E":"-0.2127402802139687","F":"2","G":"0","H":"0","I":"0","J":"non","_deepnote_index_column":9}],"rows_bottom":null},"text/plain":"                     A                    B                     C  \\\n0                 None   0.4001572083672233    0.9787379841057392   \n1   -0.977277879876411   0.9500884175255894   -0.1513572082976979   \n2    0.144043571160878    1.454273506962975    0.7610377251469934   \n3  0.33367432737426683   1.4940790731576061  -0.20515826376580087   \n4  -2.5529898158340787   0.6536185954403606    0.8644361988595057   \n5  -1.4543656745987648  0.04575851730144607   -0.1871838500258336   \n6   0.1549474256969163  0.37816251960217356   -0.8877857476301128   \n7  0.15634896910398005   1.2302906807277207    1.2023798487844113   \n8  -1.0485529650670926  -1.4200179371789752   -1.7062701906250126   \n9  -0.4380743016111864  -1.2527953600499262    0.7774903558319101   \n\n                      D                     E  F  G  H  I         J  \n0     2.240893199201458    1.8675579901499675  0  4  0  0  quisquam  \n1  -0.10321885179355784   0.41059850193837233  5  5  0  0  quisquam  \n2   0.12167501649282841   0.44386323274542566  6  1  0  0      modi  \n3   0.31306770165090136   -0.8540957393017248  0  5  0  0      eius  \n4   -0.7421650204064419    2.2697546239876076  6  7  0  0    magnam  \n5    1.5327792143584575     1.469358769900285  6  0  0  0     velit  \n6    -1.980796468223927   -0.3479121493261526  8  0  0  0    magnam  \n7   -0.3873268174079523  -0.30230275057533557  5  5  0  0      modi  \n8    1.9507753952317897   -0.5096521817516535  7  5  0  0     velit  \n9   -1.6138978475579515   -0.2127402802139687  2  0  0  0       non  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>F</th>\n      <th>G</th>\n      <th>H</th>\n      <th>I</th>\n      <th>J</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>None</td>\n      <td>0.4001572083672233</td>\n      <td>0.9787379841057392</td>\n      <td>2.240893199201458</td>\n      <td>1.8675579901499675</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>quisquam</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.977277879876411</td>\n      <td>0.9500884175255894</td>\n      <td>-0.1513572082976979</td>\n      <td>-0.10321885179355784</td>\n      <td>0.41059850193837233</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>quisquam</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.144043571160878</td>\n      <td>1.454273506962975</td>\n      <td>0.7610377251469934</td>\n      <td>0.12167501649282841</td>\n      <td>0.44386323274542566</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>modi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.33367432737426683</td>\n      <td>1.4940790731576061</td>\n      <td>-0.20515826376580087</td>\n      <td>0.31306770165090136</td>\n      <td>-0.8540957393017248</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>eius</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.5529898158340787</td>\n      <td>0.6536185954403606</td>\n      <td>0.8644361988595057</td>\n      <td>-0.7421650204064419</td>\n      <td>2.2697546239876076</td>\n      <td>6</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>magnam</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-1.4543656745987648</td>\n      <td>0.04575851730144607</td>\n      <td>-0.1871838500258336</td>\n      <td>1.5327792143584575</td>\n      <td>1.469358769900285</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>velit</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.1549474256969163</td>\n      <td>0.37816251960217356</td>\n      <td>-0.8877857476301128</td>\n      <td>-1.980796468223927</td>\n      <td>-0.3479121493261526</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>magnam</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.15634896910398005</td>\n      <td>1.2302906807277207</td>\n      <td>1.2023798487844113</td>\n      <td>-0.3873268174079523</td>\n      <td>-0.30230275057533557</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>modi</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-1.0485529650670926</td>\n      <td>-1.4200179371789752</td>\n      <td>-1.7062701906250126</td>\n      <td>1.9507753952317897</td>\n      <td>-0.5096521817516535</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>velit</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.4380743016111864</td>\n      <td>-1.2527953600499262</td>\n      <td>0.7774903558319101</td>\n      <td>-1.6138978475579515</td>\n      <td>-0.2127402802139687</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>non</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00027-89b1fca0-a882-4bf4-a3cb-7a8842bf91b6"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-8618020c-4639-4a88-b9f2-0e1212e9c7cf","output_cleared":false,"source_hash":"d56ddf52","execution_millis":470,"execution_start":1605608091018},"source":"df['J'] = pd.Categorical(df.J)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00023-cf2defbc-03c8-4b43-becd-82bc3e39cdee","output_cleared":false,"source_hash":"1ddaec9a","execution_millis":1959,"execution_start":1605608105636},"source":"%time df.to_feather('test.feather')\n","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 1.38 s, sys: 1.53 s, total: 2.91 s\nWall time: 3.11 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00024-8c3f3bc6-d8b5-4915-a22e-89d77df8f8a3","output_cleared":false,"source_hash":"3658418d","execution_millis":2699,"execution_start":1605608134600},"source":"%time df.to_parquet('test.parquet')","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 1.96 s, sys: 1.31 s, total: 3.27 s\nWall time: 3.68 s\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-7c986385-4bc6-4c82-8079-36a53df467ef","output_cleared":false,"source_hash":"a1e5a992","execution_millis":6995,"execution_start":1605608145002},"source":"%%time \ndf = pd.read_feather('test.feather')\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 3.23 s, sys: 4.41 s, total: 7.64 s\nWall time: 7 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00024-245a1c05-2399-4d6c-95f0-195be6f23e3e","output_cleared":false,"source_hash":"f37bbfe5","execution_millis":10342,"execution_start":1605608190354},"source":"%%time \ndf = pd.read_parquet('test.parquet')\nlen(df)","execution_count":null,"outputs":[{"name":"stdout","text":"CPU times: user 4.34 s, sys: 7.19 s, total: 11.5 s\nWall time: 10.4 s\n","output_type":"stream"},{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"1000000"},"metadata":{}}]},{"cell_type":"markdown","source":"## Feather or Parquet\n\n- Parquet format is designed for long-term storage, where Arrow is more intended for short term or ephemeral storage because files volume are larger.\n- Parquet is usually more expensive to write than Feather as it features more layers of encoding and compression. \n- Feather is unmodified raw columnar Arrow memory. We will probably add simple compression to Feather in the future.\n- Due to dictionary encoding, RLE encoding, and data page compression, Parquet files will often be much smaller than Feather files\n- Parquet is a standard storage format for analytics that's supported by Spark. So if you are doing analytics, Parquet is a good option as a reference storage format for query by multiple systems\n\n[source stackoverflow](https://stackoverflow.com/questions/48083405/what-are-the-differences-between-feather-and-parquet)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00029-832b0847-4adb-4f4b-8357-66febb207dda"}},{"cell_type":"markdown","source":"## Apache Arrow\n\n[Arrow](https://arrow.apache.org/docs/python/) is a columnar in-memory analytics layer designed to accelerate big data. \nIt houses a set of canonical in-memory representations of \nhierarchical data along with multiple language-bindings \nfor structure manipulation. Arrow offers an unified way to be able to \nshare the same data representation among languages and it will certainly be \nthe next standard to store dataframes in all languages.\n\n- [R package](https://cran.r-project.org/web/packages/arrow/index.html)\n- [Julia package](https://github.com/JuliaData/Arrow.jl)\n- [GitHub project](https://github.com/apache/arrow)\n\n![](images/arrow_ecosystem.png)\n\nApache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. [PyArrow](https://arrow.apache.org/docs/python/) includes Python bindings to read and write Parquet files with pandas.\n\n- columnar storage, only read the data of interest\n- efficient binary packing\n- choice of compression algorithms and encoding\n- split data into files, allowing for parallel processing\n- range of logical types\n- statistics stored in metadata allow for skipping unneeded chunks\n- data partitioning using the directory structure\n\n![arrow](images/arrow.png)\n\n- https://arrow.apache.org/docs/python/csv.html\n- https://arrow.apache.org/docs/python/feather.html\n- https://arrow.apache.org/docs/python/parquet.html\n\n\nExample:\n```py\nimport pyarrow as pa\nimport pandas as pd\nimport numpy as np\narr = np.random.randn(500000) # 10% nulls\narr[::10] = np.nan\ndf = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})\n\nhdfs = pa.hdfs.connect()\ntable = pa.Table.from_pandas(df)\npq.write_to_dataset(table, root_path=\"test\", filesystem=hdfs)\nhdfs.ls(\"test\")\n\n```\n### Read CSV from HDFS\n\nPut the file test.csv on hdfs system \n\n```python\nfrom pyarrow import csv\nwith hdfs.open(\"/data/nycflights/1999.csv\", \"rb\") as f:\n df = pd.read_csv(f, nrows = 10)\nprint(df.head())\n```\n\n### Read Parquet File from HDFS with pandas\n\n```python\nimport pandas as pd\nwikipedia = pd.read_parquet(\"hdfs://svmass2.mass.uhb.fr:54310/data/pagecounts-parquet/part-00007-8575060f-6b57-45ea-bf1d-cd77b6141f70.snappy.parquet\", engine=’pyarrow’)\nprint(wikipedia.head())\n```\n### Read Parquet File with pyarrow\n\n```py\ntable = pq.read_table(\"example.parquet\")\n```\n\n### Writing a parquet file from Apache Arrow\n```py\npq.write_table(table, \"example.parquet\")\n```\n\n### Check metadata\n```py\nparquet_file = pq.ParquetFile(\"example.parquet\")\nprint(parquet_file.metadata)\n```\n\n### See schema\n```py\nprint(parquet_file.schema)\n```\n\n### Connect to the Hadoop file system\n\n```py\nhdfs = pa.hdfs.connect()\n\n# copy to local\nwith hdfs.open(\"user.txt\", \"rb\") as f:\n    f.download(\"user.text\")\n\n# write parquet file on hdfs\nwith open(\"example.parquet\", \"rb\") as f:\n    pa.HadoopFileSystem.upload(hdfs, \"example.parquet\", f)\n\n# List files\nfor f in hdfs.ls(\"/user/navaro_p\"):\n    print(f)\n\n# create a small dataframe and write it to hadoop file system\nsmall_df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\ntable = pa.Table.from_pandas(small_df)\npq.write_table(table, \"small_df.parquet\", filesystem=hdfs)\n\n\n# Read files from Hadoop with pandas\nwith hdfs.open(\"/data/irmar.csv\") as f:\n    df = pd.read_csv(f)\n\nprint(df.head())\n\n# Read parquet file from Hadoop with pandas\nserver = \"hdfs://svmass2.mass.uhb.fr:54310\"\npath = \"data/pagecounts-parquet/part-00007-8575060f-6b57-45ea-bf1d-cd77b6141f70.snappy.parquet\"\npagecount = pd.read_parquet(os.path.join(server, path), engine=\"pyarrow\")\nprint(pagecount.head())\n\n# Read parquet file from Hadoop with pyarrow\ntable = pq.read_table(os.path.join(server,path))\nprint(table.schema)\ndf = table.to_pandas()\nprint(df.head())\n```","metadata":{"deepnote_cell_type":"markdown","cell_id":"00005-9a75a293-b946-4598-bd67-3fd8415c48c2"}},{"cell_type":"markdown","source":"### Exercise\n\n- Take the second dataframe with string as last column\n- Create an arrow table from pandas dataframe\n- Write the file test.parquet from arrow table\n- Print metadata from this parquet file\n- Print schema\n- Upload the file to hadoop file system\n- Read this file from hadoop file system and print dataframe head\n\n\nHint: check the doc https://arrow.apache.org/docs/python/parquet.html","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00031-c0409d84-12d4-42c5-adb2-ce319b58d5be"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00037-3784ae35-78ff-46a4-ab34-c9c075b05220","output_cleared":false,"source_hash":"313d4d07","execution_start":1605608783188,"execution_millis":1093},"source":"import pyarrow as pa\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np\narr = np.random.randn(500000) # 10% nulls\narr[::10] = np.nan\ndf = pd.DataFrame({'column_{0}'.format(i): arr for i in range(10)})\n\n# hdfs = pa.hdfs.connect()\ntable = pa.Table.from_pandas(df)\npq.write_to_dataset(table, root_path=\"test\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00038-1f977a10-b43e-45c5-b7d2-163bc2d57241","output_cleared":false,"source_hash":"cd45f8b2","execution_start":1605608797709,"execution_millis":147},"source":"%%bash\n\nls test","execution_count":null,"outputs":[{"name":"stdout","text":"f5bb48bcb26749878b42a265b4716fca.parquet\n","output_type":"stream"}]},{"cell_type":"markdown","source":"```python\nimport os\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport numpy as np\nimport pandas as pd\nfrom lorem import sentence\nfrom time import time\n\n\nprint(\"\"\"\n1 Creation de la dataframe avec des chaines en derniere colonne\n\"\"\")\n\nwords = np.array(sentence().strip().lower().replace(\".\", \" \").split())\n\nnp.random.seed(0)\nn = 1000000\ndf = pd.DataFrame(np.c_[np.random.randn(n, 5),\n                  np.random.randint(0,10,(n, 2)),\n                  np.random.randint(0,1,(n, 2)),\nnp.array([np.random.choice(words) for i in range(n)])] ,\ncolumns=list('ABCDEFGHIJ'))\n\ndf[\"A\"][::10] = np.nan\nprint(len(df))\n\nprint(\"\"\"\n2 Creation de la table Arrow\n\"\"\")\n\ntable = pa.Table.from_pandas(df)\n\nprint(\"\"\"\n2 Creation du fichier test.parquet depuis la table arrow\n\"\"\")\n\npq.write_table(table, \"test.parquet\")\n\nprint(\"\"\"\n3 Visualiser les metadata\n\"\"\")\n\nparquet_file = pq.ParquetFile(\"test.parquet\")\nprint(parquet_file.metadata)\nprint(\" Autre maniere de faire \")\nprint(pq.read_metadata(\"test.parquet\"))\n\nprint(\"\"\"\n4 Afficher le schema\n\"\"\")\n\nprint(parquet_file.schema)\n\nprint(\"\"\"\n5 copier le ficher parquet sur le systeme hadoop (atention c'est long)\n\"\"\")\n\nhdfs = pa.hdfs.connect()\n\nwith open(\"test.parquet\", \"rb\") as f:\n    pa.HadoopFileSystem.upload(hdfs, \"test.parquet\", f)\n\nfor f in hdfs.ls(\"/user/navaro_p\"):\n    print(f)\nserver = \"hdfs://svmass2.mass.uhb.fr:54310\"\npath = \"user/navaro_p/test.parquet\"\ntable = pq.read_table(os.path.join(server,path))\nprint(table.schema)\ndf = table.to_pandas()\nprint(df.head())\n\n```","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00039-fb75b3da-5a38-4c7f-bc3c-13107a7be153"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00018-79f4010c-254b-494c-baca-abd058d7cb85"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-d6f4c0c3-618a-4d03-9e0a-d1310f5553eb"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"big-data","language":"python","name":"big-data"},"deepnote_notebook_id":"a7088f08-45cf-4fa4-910d-a13ff12600d1","deepnote_execution_queue":[]}}