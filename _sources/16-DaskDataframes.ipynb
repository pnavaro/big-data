{"cells":[{"cell_type":"markdown","source":"# Dask Dataframes\n\n*[Dask](http://dask.pydata.org) is a flexible parallel computing library for analytic computing* written in Python. Dask is similar to Spark, by lazily constructing directed acyclic graph (DAG) of tasks and splitting large datasets into small portions called partitions. See the below image from [Dask's web page](http://dask.pydata.org) for illustration. \n\n![http://dask.pydata.org/en/latest/_images/collections-schedulers.png](images/collections-schedulers.png) \n\nIt has three main interfaces:\n\n* [Array](http://dask.pydata.org/en/latest/array.html), which works like [NumPy](http://www.numpy.org/) arrays;\n* [Bag](http://dask.pydata.org/en/latest/bag.html), which is similar to RDD interface in Spark;\n* [DataFrame](http://dask.pydata.org/en/latest/dataframe.html), which works like [Pandas](https://pandas.pydata.org/) DataFrame.\n\nWhile it can work on a [distributed cluster](http://dask.pydata.org/en/latest/distributed.html), Dask works also very well on a single cpu machine.","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00000-34d85670-67cb-4650-a768-6c9770b79793"}},{"cell_type":"markdown","source":"## DataFrames\n\nDask dataframes look and feel (mostly) like Pandas dataframes but they run on the same infrastructure that powers dask.delayed.\n\nThe `dask.dataframe` module implements a blocked parallel `DataFrame` object that mimics a large subset of the Pandas `DataFrame`. One dask `DataFrame` is comprised of many in-memory pandas `DataFrames` separated along the index. One operation on a dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n\n**Related Documentation**\n\n*  [Dask DataFrame documentation](http://dask.pydata.org/en/latest/dataframe.html)\n*  [Pandas documentation](http://pandas.pydata.org/)\n\nIn this notebook, we will extracts some historical flight data for flights out of NYC between 1990 and 2000. The data is taken from [here](http://stat-computing.org/dataexpo/2009/the-data.html). This should only take a few seconds to run.\n\nWe will use `dask.dataframe` construct our computations for us.  The `dask.dataframe.read_csv` function can take a globstring like `\"data/nycflights/*.csv\"` and build parallel computations on all of our data at once.","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00001-98701dc2-0252-40a8-88b5-3aa2ed470bf1"}},{"cell_type":"markdown","source":"### Prep the Data","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00002-9563dc0b-b721-4878-9f40-682b8a5f87fe"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00003-98696e8f-0cfa-4e84-bcf3-048a1080471d"},"source":"import os\nimport pandas as pd\npd.set_option(\"max.rows\", 10)\nos.getcwd()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00004-f9500f14-9bd5-418e-9b7b-26fc3e6e70ba"},"source":"import os  # library to get directory and file paths\nimport tarfile # this module makes possible to read and write tar archives\n\ndef extract_flight():\n    here = os.getcwd()\n    flightdir = os.path.join(here,'data', 'nycflights')\n    if not os.path.exists(flightdir):\n       print(\"Extracting flight data\")\n       tar_path = os.path.join('data', 'nycflights.tar.gz')\n       with tarfile.open(tar_path, mode='r:gz') as flights:\n          flights.extractall('data/')\n            \nextract_flight() # this function call will extract 10 csv files in data/nycflights","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data from CSVs in Dask Dataframes","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00005-1639d81b-ea9f-4747-ab77-25e62992a8d8"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00006-e3004c8e-87e0-4cb5-a9ac-85f7aeb1f15f"},"source":"import os\nhere = os.getcwd()\nfilename = os.path.join(here, 'data', 'nycflights', '*.csv')\nfilename","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00007-b4077362-2907-4a23-b17e-d94d6ad7c505"},"source":"import dask\nimport dask.dataframe as dd\n\ndf = dd.read_csv(filename,\n                 parse_dates={'Date': [0, 1, 2]})","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look to the dataframe","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00008-ec07b167-378e-44de-b55f-769cf0e3e28e"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00009-9272b1dd-71be-4144-a812-6c572eb38324"},"source":"df","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00010-1bf653eb-4f03-4c05-81df-4ed09b37b765"},"source":"### Get the first 5 rows\ndf.head()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00011-f07d5956-7ce4-4505-a159-2ca3c92e54da"},"source":"import traceback # we use traceback because we except an error.\n\ntry:\n    df.tail() # Get the last 5 rows\nexcept Exception:\n    traceback.print_exc()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What just happened?\n\nUnlike `pandas.read_csv` which reads in the entire file before inferring datatypes, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n\nIn this case, the datatypes inferred in the sample are incorrect. The first `n` rows have no value for `CRSElapsedTime` (which pandas infers as a `float`), and later on turn out to be strings (`object` dtype). When this happens you have a few options:\n\n- Specify dtypes directly using the `dtype` keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n- Increase the size of the `sample` keyword (in bytes)\n- Use `assume_missing` to make `dask` assume that columns inferred to be `int` (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply.\n\nIn our case we'll use the first option and directly specify the `dtypes` of the offending columns.","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00012-a9f45be1-856d-4684-9488-419e1f8f8a50"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00013-36e73f9c-73b6-43d2-9cb2-8135195e80e6"},"source":"df.dtypes","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00014-cc8e29fe-b30b-411a-87f2-d59b19edc718"},"source":"df = dd.read_csv(filename,\n                 parse_dates={'Date': [0, 1, 2]},\n                 dtype={'TailNum': object,\n                        'CRSElapsedTime': float,\n                        'Cancelled': bool})","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00015-3442a25d-54c0-423a-8e3c-9b0a445ebb70"},"source":"df.tail()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at one more example to fix ideas.","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00016-72ca73c1-0580-4ca5-81f9-15469a819ce1"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00017-3e13013b-57d5-4e16-8081-269fb8a8b46a"},"source":"len(df)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Why df is ten times longer ?","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00018-545003c6-f85a-41b1-9fef-e4a2ff8a9400"}},{"cell_type":"markdown","source":"- Dask investigated the input path and found that there are ten matching files. \n- A set of jobs was intelligently created for each chunk - one per original CSV file in this case. \n- Each file was loaded into a pandas dataframe, had `len()` applied to it.\n- The subtotals were combined to give you the final grant total.","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00019-c826a6c5-2551-418a-9975-865739f8f6cf"}},{"cell_type":"markdown","source":"## Computations with `dask.dataframe`\n\nWe compute the maximum of the `DepDelay` column.  With `dask.delayed` we could create this computation as follows:\n\n```python\nmaxes = []\nfor fn in filenames:\n    df = dask.delayed(pd.read_csv)(fn)\n    maxes.append(df.DepDelay.max())\n    \nfinal_max = dask.delayed(max)(maxes)\nfinal_max.compute()\n```\n\nNow we just use the normal Pandas syntax as follows:","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00020-097e684f-aa35-4ea6-b8e2-3620ae8e3d56"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00021-5354e3b4-6a0a-4d5f-a31e-77bba88f8fcd"},"source":"%time df.DepDelay.max().compute()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This writes the delayed computation for us and then runs it. Recall that the delayed computation is a dask graph made of up of key-value pairs.\n\nSome things to note:\n\n1.  As with `dask.delayed`, we need to call `.compute()` when we're done.  Up until this point everything is lazy.\n2.  Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n    -  This lets us handle datasets that are larger than memory\n    -  This means that repeated computations will have to load all of the data in each time (run the code above again, is it faster or slower than you would expect?)\n    \nAs with `Delayed` objects, you can view the underlying task graph using the `.visualize` method:","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00022-2b8e2119-5344-43a4-a938-e04e9dff91fe"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00023-9034cbd8-1e86-4bea-97bc-4bca0818c921"},"source":"df.DepDelay.max().visualize()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you are already familiar with the Pandas API then know how to use `dask.dataframe`.  There are a couple of small changes.\n\nAs noted above, computations on dask `DataFrame` objects don't perform work, instead they build up a dask graph.  We can evaluate this dask graph at any time using the `.compute()` method.","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00024-9a02b908-4d28-4166-831e-ff6c38f1a3a8"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00025-59d67226-7ba7-417b-90ca-089ab36cf728"},"source":"result = df.DepDelay.mean()  # create the tasks graph","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00026-ad1f3018-3cf5-4523-95c0-58fae36c8bff"},"source":"%time result.compute()           # perform actual computation","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Store Data in Apache Parquet Format\n\nDask encourage dataframe users to store and load data using Parquet instead. [Apache Parquet](http://parquet.apache.org/) is a columnar binary format that is easy to split into multiple files (easier for parallel loading) and is generally much simpler to deal with than HDF5 (from the Dask library’s perspective). It is also a common format used by other big data systems like [Apache Spark](http://spark.apache.org/) and [Apache Impala](http://impala.apache.org/) and so is useful to interchange with other systems.","metadata":{"cell_id":"00027-f82af3cd-9f19-48b9-8571-d4709a39ba28"}},{"cell_type":"code","metadata":{"cell_id":"00028-98add0d5-bc7e-454d-bc06-ff4cf461aefe"},"source":"df.drop(\"TailNum\", axis=1).to_parquet(\"nycflights/\")  # save csv files using parquet format","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is possible to specify dtypes and compression when converting. This can definitely help give you significantly greater speedups, but just using the default settings will still be a large improvement.","metadata":{"cell_id":"00029-7354bd32-2897-4145-b744-5732bfe1a498"}},{"cell_type":"code","metadata":{"cell_id":"00030-26109646-11c2-4118-8b7a-d87bb8ffc50e"},"source":"df.size.compute()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00031-8da17b5e-daf8-4f2e-9f65-d0384772939d"},"source":"import dask.dataframe as dd\ndf = dd.read_parquet(\"nycflights/\")\ndf.head()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00032-1c5ee145-2000-485a-a68d-cbbf5c517be9"},"source":"result = df.DepDelay.mean() ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00033-bb79077a-7c7b-4dc2-8cbf-5da748056e5b"},"source":"%time result.compute() ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The computation is much faster because pulling out the DepDelay column is  easy for Parquet.\n\n### Parquet advantages:\n\n- Binary representation of data, allowing for speedy conversion of bytes-on-disk to bytes-in-memory\n- Columnar storage, meaning that you can load in as few columns as you need without loading the entire dataset\n- Row-chunked storage so that you can pull out data from a particular range without touching the others\n- Per-chunk statistics so that you can find subsets quickly\n- Compression","metadata":{"cell_id":"00034-35e6ef2c-0dad-4501-8ac3-36794f974eed"}},{"cell_type":"markdown","source":"### Exercise 15.1","metadata":{"cell_id":"00035-a4f6b200-fe7d-443f-8290-4f490d9d1dbc"}},{"cell_type":"markdown","source":"If you don't remember how to use pandas.  Please read [pandas documentation](http://pandas.pydata.org/).","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00036-789ec646-adbc-41d6-bc8a-d72223c23923"}},{"cell_type":"markdown","source":"- Use the `head()` method to get the first ten rows\n- How many rows are in our dataset?\n- Use selections `df[...]` to find how many positive (late) and negative (early) departure times there are\n- In total, how many non-cancelled flights were taken? (To invert a boolean pandas Series s, use ~s).","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00037-887d9aab-0973-4f2e-983d-950c5f32d76d"}},{"cell_type":"markdown","source":"Divisions and the Index\n---------------------------\n\nThe Pandas index associates a value to each record/row of your data.  Operations that align with the index, like `loc` can be a bit faster as a result.\n\nIn `dask.dataframe` this index becomes even more important.  Recall that one dask `DataFrame` consists of several Pandas `DataFrame`s.  These dataframes are separated along the index by value.  For example, when working with time series we may partition our large dataset by month.\n\nRecall that these many partitions of our data may not all live in memory at the same time, instead they might live on disk; we simply have tasks that can materialize these pandas `DataFrames` on demand.\n\nPartitioning your data can greatly improve efficiency.  Operations like `loc`, `groupby`, and `merge/join` along the index are *much more efficient* than operations along other columns.  You can see how your dataset is partitioned with the `.divisions` attribute.  Note that data that comes out of simple data sources like CSV files aren't intelligently indexed by default.  In these cases the values for `.divisions` will be `None.`","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00038-6870dbe7-ecb6-41ed-92a6-63e058b09761"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00039-d7feae35-75c1-4573-acd5-9de3dc6e4d77"},"source":"df = dd.read_csv(filename,\n                 dtype={'TailNum': str,\n                        'CRSElapsedTime': float,\n                        'Cancelled': bool})\ndf.divisions","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However if we set the index to some new column then dask will divide our data roughly evenly along that column and create new divisions for us.  Warning, `set_index` triggers immediate computation.","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00040-c527ef76-ca6b-4d88-a68c-392c466d1fa0"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00041-ebfa4c7d-e575-463c-99b7-2e0c7c2ac36b"},"source":"df2 = df.set_index('Year')\ndf2.divisions","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see here the minimum and maximum values (1990 and 1999) as well as the intermediate values that separate our data well.  This dataset has ten partitions, as the final value is assumed to be the inclusive right-side for the last bin.","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00042-8158844f-8db9-49eb-a575-d726b7282072"}},{"cell_type":"code","metadata":{"cell_id":"00043-17cae8b0-2269-4f66-847f-1a96b9c03e3d"},"source":"df2.npartitions","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00044-19b61811-99e0-483d-a96c-df70b89f8870"},"source":"df2.head()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the benefits of this is that operations like `loc` only need to load the relevant partitions","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00045-9bf19ceb-ae31-414c-84d2-6bda6ef1b7db"}},{"cell_type":"code","metadata":{"cell_id":"00046-9f0ff23d-b535-4ab6-aa57-83bee6ccf54f"},"source":"df2.loc[1991]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00047-2b50b8d3-aa0f-49c1-b80e-3f75e75f54bc"},"source":"df2.loc[1991].compute()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercises 15.2\n\nIn this section we do a few `dask.dataframe` computations. If you are comfortable with Pandas then these should be familiar. You will have to think about when to call `compute`.\n\n- In total, how many non-cancelled flights were taken from each airport?\n\n*Hint*: use [`df.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html). `df.groupby(df.A).B.func()`.\n\n- What was the average departure delay from each airport?\n\nNote, this is the same computation you did in the previous notebook (is this approach faster or slower?)\n\n- What day of the week has the worst average departure delay?","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00048-f7357e34-8794-4c13-9592-41c95dbbb109"}},{"cell_type":"markdown","source":"## Sharing Intermediate Results\n\nWhen computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n\nFor example, lets compute the mean and standard deviation for departure delay of all non-cancelled flights:","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00049-cff71ef9-fbb1-42bd-90cb-9a365a387a0b"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00050-46264025-ae09-4489-a1d6-f039cce003d5"},"source":"non_cancelled = df[~df.Cancelled]\nmean_delay = non_cancelled.DepDelay.mean()\nstd_delay = non_cancelled.DepDelay.std()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using two calls to `.compute`:","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00051-0dc754e4-3cf4-4f8b-87d3-ad1d229f736b"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00052-adf21bd3-f018-4437-9664-70c02525f4c3"},"source":"%%time\nmean_delay_res = mean_delay.compute()\nstd_delay_res = std_delay.compute()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using one call to `dask.compute`:","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00053-c4ce9fe3-4b08-4ddd-aedc-f93eeb17bc35"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"fragment"},"cell_id":"00054-92653e21-ae5f-4729-a245-755720be1b58"},"source":"%%time\nmean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n\n- the calls to `read_csv`\n- the filter (`df[~df.Cancelled]`)\n- some of the necessary reductions (`sum`, `count`)\n\nTo see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function (we might want to use `filename='graph.pdf'` to zoom in on the graph better):","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00055-1dc42500-6f21-4b4a-b5ba-36dba1578e33"}},{"cell_type":"code","metadata":{"slideshow":{"slide_type":"slide"},"cell_id":"00056-35300028-b67b-431c-8654-8a776a40708b"},"source":"dask.visualize(mean_delay, std_delay)","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"jupytext":{"cell_metadata_json":true,"encoding":"# -*- coding: utf-8 -*-"},"kernelspec":{"display_name":"big-data","language":"python","name":"big-data"},"deepnote_notebook_id":"1cdc24f6-af29-490e-bed5-7ee13253846d","deepnote_execution_queue":[]}}