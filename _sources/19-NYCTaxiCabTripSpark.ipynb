{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-fed2f096-7d65-49d7-9e26-b534726f0581",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# New York City Taxi Cab Trip - Spark dataframes on HDFS\n",
    "\n",
    "\n",
    "\n",
    "We look at [the New York City Taxi Cab dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). This includes every ride made in the city of New York since 2009.\n",
    "\n",
    "On [this website](http://chriswhong.github.io/nyctaxi/) you can see the data for one random NYC yellow taxi on a single day.\n",
    "\n",
    "On [this post](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/), you can see an analysis of this dataset. Postgres and R scripts are available on [GitHub](https://github.com/toddwschneider/nyc-taxi-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-3cd6a9c1-5054-4e7c-b0d1-e4e348683ddd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "Normally we would read and load this data into memory as a Pandas dataframe.  However in this case that would be unwise because this data is too large to fit in RAM.\n",
    "\n",
    "The data can stay in the hdfs filesystem but for performance reason we can't use the csv format. The file is large (32Go) and text formatted. Data Access is very slow.\n",
    "\n",
    "You can convert csv file to parquet with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Convert CSV to parquet\") \\\n",
    "        .master(\"spark://b2-120-gra11:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"hdfs://b2-120-gra11:54310/data/nyctaxi/2018/yellow*.csv\", \n",
    "                    header=\"true\",inferSchema=\"true\")\n",
    "\n",
    "df.write.parquet(\"hdfs://b2-120-gra11:54310/user/jupyter-navaro_p/2018-yellow.parquet\")\n",
    "\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-64de320b-50aa-4928-9884-23b77f541459",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Spark Cluster\n",
    "\n",
    "A Spark cluster is available and described on this [web interface](http://mas-sdd-rennes2.ovh:8080)\n",
    "\n",
    "![](images/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-64de320b-50aa-4928-9884-23b77f541459",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The SparkSession is connected to the Spark’s own standalone cluster manager (It is also possible to use YARN). The manager allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (Python file) to the executors. Finally, tasks are sent to the executors to run.\n",
    "\n",
    "Spark can access to files located on hdfs and it is also possible to access to local files. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark-submit.sh\n",
    "\n",
    "The spark-submit script is used to launch applications on a cluster. It can use all of Spark’s supported cluster managers through a uniform interface so you don’t have to configure your application especially for each one.\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "\n",
    "import sys, os\n",
    " \n",
    "from pyspark import SparkContext, SparkConf\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    # create Spark context with Spark configuration\n",
    "    conf = SparkConf().setAppName(\"WordCount\")\n",
    "    sc = SparkContext(conf=conf)\n",
    " \n",
    "    # read in text file and split each document into words\n",
    "    words = sc.textFile(\"file:///srv/data/sample.txt\").flatMap(lambda line: line.strip().lower().replace(\".\",\"\").split(\" \"))\n",
    " \n",
    "    # count the occurrence of each word\n",
    "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\n",
    " \n",
    "    wordCounts.saveAsTextFile(os.path.join(\"output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tester la commande en l'exécutant dans un terminal\n",
    "\n",
    "```bash\n",
    "/opt/spark-3.3.1-bin-hadoop3/bin/spark-submit wordcount.py\n",
    "```\n",
    "\n",
    "Utilisez `Ctrl-C` pour la stopper en cas de problème\n",
    "\n",
    "par défaut la sortie se fait sur le système de fichier hdfs \n",
    "\n",
    "```\n",
    "hdfs dfs -cat output/*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-64de320b-50aa-4928-9884-23b77f541459",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- **Don't run the python code inside a notebook cell**. Save a python script and launch it from a terminal instead.\n",
    "In Jupyter notebook you won't see any progress or information if error occurs.\n",
    "- Documentation of [`spark-submit`](https://spark.apache.org/docs/latest/submitting-applications.html) command shell to run your script on the cluster.\n",
    "- You can control the log with \n",
    "```py\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "```\n",
    "Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
    "\n",
    "**Try your script with a single file before to do it for whole data.**\n",
    "\n",
    "**Read carefully your script before, don't submit many times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Some examples that can be run on the cluster\n",
    "\n",
    "- Here we read the NYC taxi data files of year 2018 and select some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00003-64de320b-50aa-4928-9884-23b77f541459",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mas-sdd-rennes2.ovh:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://b2-120-gra11:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe771fdb220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master('spark://b2-120-gra11:7077') \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00003-64de320b-50aa-4928-9884-23b77f541459",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns = ['tpep_pickup_datetime', 'passenger_count', 'payment_type', 'fare_amount', \n",
    "           'tip_amount', 'total_amount']\n",
    "df = (spark.read.parquet(\"hdfs://localhost:54310/data/nyctaxi/2018.parquet/\").select(*columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- Sum the total number of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                        (0 + 48) / 68]\r"
     ]
    }
   ],
   "source": [
    "df.agg({'passenger_count': 'sum'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- Average number of passenger per trip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [],
   "source": [
    "df.agg({'passenger_count': 'avg'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "- How many trip with 0,1,2,3,...,9 passenger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00004-1b1bb60b-720d-42f8-8643-7b4812088e73",
    "deepnote_cell_type": "markdown"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(passenger_count=1, count(1)=73072141),\n",
       " Row(passenger_count=6, count(1)=2783068),\n",
       " Row(passenger_count=3, count(1)=4295075),\n",
       " Row(passenger_count=5, count(1)=4602861),\n",
       " Row(passenger_count=9, count(1)=275),\n",
       " Row(passenger_count=4, count(1)=2029082),\n",
       " Row(passenger_count=8, count(1)=313),\n",
       " Row(passenger_count=7, count(1)=390),\n",
       " Row(passenger_count=2, count(1)=15087976),\n",
       " Row(passenger_count=0, count(1)=933067),\n",
       " Row(passenger_count=96, count(1)=1),\n",
       " Row(passenger_count=192, count(1)=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('passenger_count').agg({'*': 'count'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-cbfd6d94-01f1-40b7-b047-d662d76f6b88",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Example\n",
    "\n",
    "How well people tip based on the number of passengers in a cab.  To do this you have to:\n",
    "\n",
    "1.  Remove rides with zero fare\n",
    "2.  Add a new column `tip_fraction` that is equal to the ratio of the tip to the fare\n",
    "3.  Group by the `passenger_count` column and take the mean of the `tip_fraction` column.\n",
    "\n",
    "- To remove rows\n",
    "```python\n",
    "df = df.filter(df.name == 'expression')\n",
    "```\n",
    "\n",
    "- To make new columns\n",
    "```python\n",
    "df = df.withColumn('var2', df.var0 + df.var1)\n",
    "```\n",
    "\n",
    "- To do groupby-aggregations\n",
    "```python\n",
    "df.groupBy(df.name).agg({'column-name': 'avg'})\n",
    "```\n",
    "\n",
    "When you want to collect the result of your computation, finish with the `.collect()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(passenger_count=1, avg(tip_fraction)=0.16703534839637652),\n",
       " Row(passenger_count=6, avg(tip_fraction)=0.14819715619095095),\n",
       " Row(passenger_count=3, avg(tip_fraction)=0.15911954017995542),\n",
       " Row(passenger_count=5, avg(tip_fraction)=0.15291846582994303),\n",
       " Row(passenger_count=9, avg(tip_fraction)=0.16675724210066872),\n",
       " Row(passenger_count=4, avg(tip_fraction)=0.15219073673118927),\n",
       " Row(passenger_count=8, avg(tip_fraction)=0.2569091162309514),\n",
       " Row(passenger_count=7, avg(tip_fraction)=0.11830764381318884),\n",
       " Row(passenger_count=2, avg(tip_fraction)=0.15288115804135563),\n",
       " Row(passenger_count=0, avg(tip_fraction)=0.1687758008957535),\n",
       " Row(passenger_count=192, avg(tip_fraction)=0.24333333333333332),\n",
       " Row(passenger_count=96, avg(tip_fraction)=0.22666666666666668)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 10:18:04 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/12/06 10:18:04 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:218)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:923)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/12/06 10:18:04 ERROR TaskSchedulerImpl: Lost executor 7 on 141.94.168.194: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "(df.filter(df.fare_amount > 0)\n",
    "   .withColumn('tip_fraction', df.tip_amount / df.fare_amount)\n",
    "   .groupby('passenger_count').agg({'tip_fraction': 'avg'})).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "87fd115f-25ed-48ed-ad5a-7ae17a9eb4bc",
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
