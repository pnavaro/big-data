{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-e91b1ad7-05a5-48c2-8c81-523c66e9b15f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "- Enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing\n",
    "- Inspired by data frames in R and Python (Pandas)\n",
    "- Designed from the ground-up to support modern big\n",
    "data and data science applications\n",
    "- Extension to the existing RDD API\n",
    "\n",
    "## References\n",
    "- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n",
    "- [PySpark Cheat Sheet: Spark DataFrames in Python](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-66b4726d-c84b-4710-baab-d780d1bf8f03",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### DataFrames are :\n",
    "- The preferred abstraction in Spark\n",
    "- Strongly typed collection of distributed elements \n",
    "- Built on Resilient Distributed Datasets (RDD)\n",
    "- Immutable once constructed\n",
    "\n",
    "### With Dataframes you can :\n",
    "- Track lineage information to efficiently recompute lost data \n",
    "- Enable operations on collection of elements in parallel\n",
    "\n",
    "### You construct DataFrames\n",
    "- by parallelizing existing collections (e.g., Pandas DataFrames) \n",
    "- by transforming an existing DataFrames\n",
    "- from files in HDFS or any other storage system (e.g., Parquet)\n",
    "\n",
    "### Features\n",
    "- Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster\n",
    "- Support for a wide array of data formats and storage systems\n",
    "- Seamless integration with all big data tooling and infrastructure via Spark\n",
    "- APIs for Python, Java, Scala, and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-6f9f0611-f0ae-4617-b092-fd5be36cd0d5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### DataFrames versus RDDs\n",
    "- Nice API for new users familiar with data frames in other programming languages.\n",
    "- For existing Spark users, the API will make Spark easier to program than using RDDs\n",
    "- For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-39edf472-0517-4cf9-82c5-dc2d34bbbb9e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## PySpark Shell\n",
    "\n",
    "**Run the Spark shell:**\n",
    "\n",
    "~~~ bash\n",
    "pyspark\n",
    "~~~\n",
    "\n",
    "Output similar to the following will be displayed, followed by a `>>>` REPL prompt:\n",
    "\n",
    "~~~\n",
    "Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n",
    "[GCC 7.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "2018-09-18 17:13:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "~~~\n",
    "\n",
    "Read data and convert to Dataset\n",
    "\n",
    "~~~ py\n",
    "df = sqlContext.read.csv(\"/tmp/irmar.csv\", sep=';', header=True)\n",
    "~~~\n",
    "\n",
    "~~~\n",
    ">>> df2.show()\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "|_c0|                name|       phone|office|organization|position|  hdr|    team1|   team2|\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "|  0|      Alphonse Paul |+33223235223|   214|          R1|     DOC|False|      EDP|      NA|\n",
    "|  1|        Ammari Zied |+33223235811|   209|          R1|      MC| True|      EDP|      NA|\n",
    ".\n",
    ".\n",
    ".\n",
    "| 18|    Bernier Joachim |+33223237558|   214|          R1|     DOC|False|   ANANUM|      NA|\n",
    "| 19|   Berthelot Pierre |+33223236043|   601|          R1|      PE| True|       GA|      NA|\n",
    "+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n",
    "only showing top 20 rows\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-3f3366d4-f0f6-4aee-b495-0499a2d9058b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Transformations, Actions, Laziness\n",
    "\n",
    "Like RDDs, DataFrames are lazy. Transformations contribute to the query plan, but they don't execute anything.\n",
    "Actions cause the execution of the query.\n",
    "\n",
    "### Transformation examples\n",
    "- filter\n",
    "- select\n",
    "- drop\n",
    "- intersect \n",
    "- join\n",
    "### Action examples\n",
    "- count \n",
    "- collect \n",
    "- show \n",
    "- head\n",
    "- take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-bfc3fb41-eb11-44c7-a9ea-f5bb47b65b78",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Creating a DataFrame in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-5a6c7399-651e-433e-8f93-ceb2c87b79f0",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-d9d415d1-5949-4ece-9e73-251bca9c98ca",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "# The following three lines are not necessary\n",
    "# in the pyspark shell\n",
    "conf = SparkConf().setAppName(\"people\").setMaster(\"local[*]\") \n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-02e6ee97-82ff-4525-80ae-bc20808d952a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"data/people.json\") # get a dataframe from json file\n",
    "\n",
    "df.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-285daf5f-09a2-43fe-91d3-8dbc9d12fb30",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Schema Inference\n",
    "\n",
    "In this exercise, let's explore schema inference. We're going to be using a file called `irmar.txt`. The data is structured, but it has no self-describing schema. And, it's not JSON, so Spark can't infer the schema automatically. Let's create an RDD and look at the first few rows of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-6542567c-7758-4453-9878-ad75618a9b20",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/irmar.csv\")\n",
    "for line in rdd.take(10):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-a25cbdca-7454-49f2-980c-4f701161b55d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Hands-on Exercises\n",
    "\n",
    "You can look at the <a href=\"http://spark.apache.org/docs/2.3.1/api/python/index.html\" target=\"_blank\">DataFrames API documentation</a> \n",
    "\n",
    "Let's take a look to file \"/tmp/irmar.csv\". Each line consists \n",
    "of the same information about a person:\n",
    "\n",
    "* name\n",
    "* phone\n",
    "* office\n",
    "* organization\n",
    "* position \n",
    "* hdr\n",
    "* team1\n",
    "* team2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-8cfe56f0-c5e8-4d83-a2fa-3e85108f624a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "rdd = sc.textFile(\"data/irmar.csv\")\n",
    "\n",
    "Person = namedtuple('Person', ['name', 'phone', 'office', 'organization', \n",
    "                               'position', 'hdr', 'team1', 'team2'])\n",
    "def str_to_bool(s):\n",
    "    if s == 'True': return True\n",
    "    return False\n",
    "\n",
    "def map_to_person(line):\n",
    "    cols = line.split(\";\")\n",
    "    return Person(name         = cols[0],\n",
    "                  phone        = cols[1],\n",
    "                  office       = cols[2],\n",
    "                  organization = cols[3],\n",
    "                  position     = cols[4], \n",
    "                  hdr          = str_to_bool(cols[5]),\n",
    "                  team1        = cols[6],\n",
    "                  team2        = cols[7])\n",
    "    \n",
    "people_rdd = rdd.map(map_to_person)\n",
    "df = people_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00013-242264f2-6446-49b0-99a8-aaf0a119370b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-d2552da3-0142-48fb-b314-541535086444",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-11feeecf-9cd9-4938-aa53-55443edfb762",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-9c1862a5-de80-4165-ac30-923629b2aeeb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-40b484a1-55ae-4332-b8f9-ef28e752db55",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-3efcd17e-653f-4503-93e6-a89581e5e155",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-5bb4b7ac-ea01-414d-8c6d-8bf81791ff42",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.select(df[\"name\"], df[\"position\"], df[\"organization\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-0c1cd8b8-fa29-4218-a26b-6944adb3b36f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.select(df[\"name\"], df[\"position\"], df[\"organization\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-96d671e3-13ee-4a6f-ab1e-ad2fe0c1d461",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-78b374b3-01a0-4781-a3d0-2dca1e7adeaa",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"organization\"] == \"R2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-d5f0c0c3-7238-4dc7-ad0c-3e055fcd535c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### filter + select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-43c070dc-7b1c-4156-8662-b1763a1be2ed",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df2 = df.filter(df[\"organization\"] == \"R2\").select(df['name'],df['team1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-4a2c47fd-40e4-42f6-9ec6-4d0696b87d38",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-1d0ac1eb-0ba8-4d76-aec1-94af3cd35b4d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-758d72cc-21a5-493b-9515-3b09f460bcd2",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "(df.filter(df[\"organization\"] == \"R2\")\n",
    "   .select(df[\"name\"],df[\"position\"])\n",
    "   .orderBy(\"position\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-7a875b74-ac41-493b-8512-c830dc79cb5d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00029-012ad184-07da-48a9-ac24-e38514d30fa0",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.groupby(df[\"hdr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-c6ba2b2b-fb0f-49ba-bab5-2bf1b962d885",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.groupby(df[\"hdr\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-02ae969f-21c9-4228-a23a-5742d4932f4b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "WARNING: Don't confuse GroupedData.count() with DataFrame.count(). GroupedData.count() is not an action. DataFrame.count() is an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-cdde6dcf-628c-4caa-b984-cfba0dff5afc",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"hdr\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00033-46281461-bbd0-41c9-bb11-6b2c1edb768f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.filter(df['hdr']).select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-24e8c678-7489-43e7-92ca-9b6397cd6e48",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "df.groupBy(df[\"organization\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-103019c3-704e-4146-a935-8cb3c7cc789b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Exercises\n",
    "\n",
    "- How many teachers from INSA (PR+MC) ?\n",
    "- How many MC in STATS team ?\n",
    "- How many MC+CR with HDR ?\n",
    "- What is the ratio of student supervision (DOC / HDR) ?\n",
    "- List number of people for every organization ?\n",
    "- List number of HDR people for every team ?\n",
    "- Which team contains most HDR ?\n",
    "- List number of DOC students for every organization ?\n",
    "- Which team contains most DOC ?\n",
    "- List people from CNRS that are neither CR nor DR ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-317e204d-c3ec-4549-9285-2dee794a1e0e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-30a3c2e8-b122-46be-b62c-efe14c2e71df",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "34558ca3-99ac-48c4-8d6f-270fc26fe9dc",
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "big-data"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
