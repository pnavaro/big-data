{"cells":[{"cell_type":"markdown","source":"# Spark DataFrames\n\n- Enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing\n- Inspired by data frames in R and Python (Pandas)\n- Designed from the ground-up to support modern big\ndata and data science applications\n- Extension to the existing RDD API\n\n## References\n- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n- [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n- [PySpark Cheat Sheet: Spark DataFrames in Python](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)","metadata":{"cell_id":"00000-e91b1ad7-05a5-48c2-8c81-523c66e9b15f"}},{"cell_type":"markdown","source":"### DataFrames are :\n- The preferred abstraction in Spark\n- Strongly typed collection of distributed elements \n- Built on Resilient Distributed Datasets (RDD)\n- Immutable once constructed\n\n### With Dataframes you can :\n- Track lineage information to efficiently recompute lost data \n- Enable operations on collection of elements in parallel\n\n### You construct DataFrames\n- by parallelizing existing collections (e.g., Pandas DataFrames) \n- by transforming an existing DataFrames\n- from files in HDFS or any other storage system (e.g., Parquet)\n\n### Features\n- Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster\n- Support for a wide array of data formats and storage systems\n- Seamless integration with all big data tooling and infrastructure via Spark\n- APIs for Python, Java, Scala, and R","metadata":{"cell_id":"00001-66b4726d-c84b-4710-baab-d780d1bf8f03"}},{"cell_type":"markdown","source":"### DataFrames versus RDDs\n- Nice API for new users familiar with data frames in other programming languages.\n- For existing Spark users, the API will make Spark easier to program than using RDDs\n- For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation","metadata":{"cell_id":"00002-6f9f0611-f0ae-4617-b092-fd5be36cd0d5"}},{"cell_type":"markdown","source":"## PySpark Shell\n\n**Run the Spark shell:**\n\n~~~ bash\npyspark\n~~~\n\nOutput similar to the following will be displayed, followed by a `>>>` REPL prompt:\n\n~~~\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)\n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n2018-09-18 17:13:13 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Python version 3.6.5 (default, Apr 29 2018 16:14:56)\nSparkSession available as 'spark'.\n>>>\n~~~\n\nRead data and convert to Dataset\n\n~~~ py\ndf = sqlContext.read.csv(\"/tmp/irmar.csv\", sep=';', header=True)\n~~~\n\n~~~\n>>> df2.show()\n+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n|_c0|                name|       phone|office|organization|position|  hdr|    team1|   team2|\n+---+--------------------+------------+------+------------+--------+-----+---------+--------+\n|  0|      Alphonse Paul |+33223235223|   214|          R1|     DOC|False|      EDP|      NA|\n|  1|        Ammari Zied |+33223235811|   209|          R1|      MC| True|      EDP|      NA|\n.\n.\n.\n| 18|    Bernier Joachim |+33223237558|   214|          R1|     DOC|False|   ANANUM|      NA|\n| 19|   Berthelot Pierre |+33223236043|   601|          R1|      PE| True|       GA|      NA|\n+---+--------------------+------------+------+------------+--------+-----+---------+--------+\nonly showing top 20 rows\n~~~","metadata":{"cell_id":"00003-39edf472-0517-4cf9-82c5-dc2d34bbbb9e"}},{"cell_type":"markdown","source":"## Transformations, Actions, Laziness\n\nLike RDDs, DataFrames are lazy. Transformations contribute to the query plan, but they don't execute anything.\nActions cause the execution of the query.\n\n### Transformation examples\n- filter\n- select\n- drop\n- intersect \n- join\n### Action examples\n- count \n- collect \n- show \n- head\n- take","metadata":{"cell_id":"00004-3f3366d4-f0f6-4aee-b495-0499a2d9058b"}},{"cell_type":"markdown","source":"## Creating a DataFrame in Python","metadata":{"cell_id":"00005-bfc3fb41-eb11-44c7-a9ea-f5bb47b65b78"}},{"cell_type":"code","metadata":{"cell_id":"00006-5a6c7399-651e-433e-8f93-ceb2c87b79f0"},"source":"import sys, subprocess\nimport os\n\nos.environ[\"PYSPARK_PYTHON\"] = sys.executable","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00007-d9d415d1-5949-4ece-9e73-251bca9c98ca"},"source":"from pyspark import SparkContext, SparkConf, SQLContext\n# The following three lines are not necessary\n# in the pyspark shell\nconf = SparkConf().setAppName(\"people\").setMaster(\"local[*]\") \nsc = SparkContext(conf=conf)\nsc.setLogLevel(\"ERROR\")\nsqlContext = SQLContext(sc)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00008-02e6ee97-82ff-4525-80ae-bc20808d952a"},"source":"df = sqlContext.read.json(\"data/people.json\") # get a dataframe from json file\n\ndf.show(24)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Schema Inference\n\nIn this exercise, let's explore schema inference. We're going to be using a file called `irmar.txt`. The data is structured, but it has no self-describing schema. And, it's not JSON, so Spark can't infer the schema automatically. Let's create an RDD and look at the first few rows of the file.","metadata":{"cell_id":"00009-285daf5f-09a2-43fe-91d3-8dbc9d12fb30"}},{"cell_type":"code","metadata":{"cell_id":"00010-6542567c-7758-4453-9878-ad75618a9b20"},"source":"rdd = sc.textFile(\"data/irmar.csv\")\nfor line in rdd.take(10):\n  print(line)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hands-on Exercises\n\nYou can look at the <a href=\"http://spark.apache.org/docs/2.3.1/api/python/index.html\" target=\"_blank\">DataFrames API documentation</a> \n\nLet's take a look to file \"/tmp/irmar.csv\". Each line consists \nof the same information about a person:\n\n* name\n* phone\n* office\n* organization\n* position \n* hdr\n* team1\n* team2","metadata":{"cell_id":"00011-a25cbdca-7454-49f2-980c-4f701161b55d"}},{"cell_type":"code","metadata":{"cell_id":"00012-8cfe56f0-c5e8-4d83-a2fa-3e85108f624a"},"source":"from collections import namedtuple\n\nrdd = sc.textFile(\"data/irmar.csv\")\n\nPerson = namedtuple('Person', ['name', 'phone', 'office', 'organization', \n                               'position', 'hdr', 'team1', 'team2'])\ndef str_to_bool(s):\n    if s == 'True': return True\n    return False\n\ndef map_to_person(line):\n    cols = line.split(\";\")\n    return Person(name         = cols[0],\n                  phone        = cols[1],\n                  office       = cols[2],\n                  organization = cols[3],\n                  position     = cols[4], \n                  hdr          = str_to_bool(cols[5]),\n                  team1        = cols[6],\n                  team2        = cols[7])\n    \npeople_rdd = rdd.map(map_to_person)\ndf = people_rdd.toDF()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00013-242264f2-6446-49b0-99a8-aaf0a119370b"},"source":"df.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Schema","metadata":{"cell_id":"00014-d2552da3-0142-48fb-b314-541535086444"}},{"cell_type":"code","metadata":{"cell_id":"00015-11feeecf-9cd9-4938-aa53-55443edfb762"},"source":"df.printSchema()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### display","metadata":{"cell_id":"00016-9c1862a5-de80-4165-ac30-923629b2aeeb"}},{"cell_type":"code","metadata":{"cell_id":"00017-40b484a1-55ae-4332-b8f9-ef28e752db55"},"source":"display(df)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### select","metadata":{"cell_id":"00018-3efcd17e-653f-4503-93e6-a89581e5e155"}},{"cell_type":"code","metadata":{"cell_id":"00019-5bb4b7ac-ea01-414d-8c6d-8bf81791ff42"},"source":"df.select(df[\"name\"], df[\"position\"], df[\"organization\"])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00020-0c1cd8b8-fa29-4218-a26b-6944adb3b36f"},"source":"df.select(df[\"name\"], df[\"position\"], df[\"organization\"]).show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### filter","metadata":{"cell_id":"00021-96d671e3-13ee-4a6f-ab1e-ad2fe0c1d461"}},{"cell_type":"code","metadata":{"cell_id":"00022-78b374b3-01a0-4781-a3d0-2dca1e7adeaa"},"source":"df.filter(df[\"organization\"] == \"R2\").show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### filter + select","metadata":{"cell_id":"00023-d5f0c0c3-7238-4dc7-ad0c-3e055fcd535c"}},{"cell_type":"code","metadata":{"cell_id":"00024-43c070dc-7b1c-4156-8662-b1763a1be2ed"},"source":"df2 = df.filter(df[\"organization\"] == \"R2\").select(df['name'],df['team1'])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00025-4a2c47fd-40e4-42f6-9ec6-4d0696b87d38"},"source":"df2.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### orderBy","metadata":{"cell_id":"00026-1d0ac1eb-0ba8-4d76-aec1-94af3cd35b4d"}},{"cell_type":"code","metadata":{"cell_id":"00027-758d72cc-21a5-493b-9515-3b09f460bcd2"},"source":"(df.filter(df[\"organization\"] == \"R2\")\n   .select(df[\"name\"],df[\"position\"])\n   .orderBy(\"position\")).show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### groupBy","metadata":{"cell_id":"00028-7a875b74-ac41-493b-8512-c830dc79cb5d"}},{"cell_type":"code","metadata":{"cell_id":"00029-012ad184-07da-48a9-ac24-e38514d30fa0"},"source":"df.groupby(df[\"hdr\"])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00030-c6ba2b2b-fb0f-49ba-bab5-2bf1b962d885"},"source":"df.groupby(df[\"hdr\"]).count().show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WARNING: Don't confuse GroupedData.count() with DataFrame.count(). GroupedData.count() is not an action. DataFrame.count() is an action.","metadata":{"cell_id":"00031-02ae969f-21c9-4228-a23a-5742d4932f4b"}},{"cell_type":"code","metadata":{"cell_id":"00032-cdde6dcf-628c-4caa-b984-cfba0dff5afc"},"source":"df.filter(df[\"hdr\"]).count()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00033-46281461-bbd0-41c9-bb11-6b2c1edb768f"},"source":"df.filter(df['hdr']).select(\"name\").show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00034-24e8c678-7489-43e7-92ca-9b6397cd6e48"},"source":"df.groupBy(df[\"organization\"]).count().show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exercises\n\n- How many teachers from INSA (PR+MC) ?\n- How many MC in STATS team ?\n- How many MC+CR with HDR ?\n- What is the ratio of student supervision (DOC / HDR) ?\n- List number of people for every organization ?\n- List number of HDR people for every team ?\n- Which team contains most HDR ?\n- List number of DOC students for every organization ?\n- Which team contains most DOC ?\n- List people from CNRS that are neither CR nor DR ?","metadata":{"cell_id":"00035-103019c3-704e-4146-a935-8cb3c7cc789b"}},{"cell_type":"code","metadata":{"cell_id":"00036-317e204d-c3ec-4549-9285-2dee794a1e0e"},"source":"sc.stop()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00037-30a3c2e8-b122-46be-b62c-efe14c2e71df"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"jupytext":{"encoding":"# -*- coding: utf-8 -*-"},"kernelspec":{"display_name":"big-data","language":"python","name":"big-data"},"deepnote_notebook_id":"34558ca3-99ac-48c4-8d6f-270fc26fe9dc","deepnote_execution_queue":[]}}